---
# Source: pgadmin4/templates/networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
spec:
  policyTypes:
    - Ingress
  podSelector:
    matchLabels:
      app.kubernetes.io/name: pgadmin4
      app.kubernetes.io/instance: pgadmin4
  ingress:
  - ports:
    - port: 80
---
# Source: pgadmin4/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  password: "U3VwZXJTZWNyZXQ="
---
# Source: pgadmin4/templates/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
# Source: pgadmin4/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: pgadmin4
    app.kubernetes.io/instance: pgadmin4
---
# Source: pgadmin4/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: pgadmin4
      app.kubernetes.io/instance: pgadmin4
  template:
    metadata:
      labels:
        app.kubernetes.io/name: pgadmin4
        app.kubernetes.io/instance: pgadmin4
      annotations:
        checksum/secret: 2acaa91f3a0987d847c541fcd7622f6419a80506088a14f3e4821dbb16fd3a9f

    spec:
      containers:
        - name: pgadmin4
          image: "docker.io/dpage/pgadmin4:7.4"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              port: http
              path: "/pgadmin4/misc/ping"
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 15
          readinessProbe:
            httpGet:
              port: http
              path: "/pgadmin4/misc/ping"
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 15
          env:
            - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION
              value: "False"
            - name: PGADMIN_DEFAULT_EMAIL
              value: chart@domain.com
            - name: PGADMIN_DEFAULT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: pgadmin4
                  key: password
            - name: SCRIPT_NAME
              value: /pgadmin4
          volumeMounts:
            - name: pgadmin-data
              mountPath: /var/lib/pgadmin
          resources:
            {}
      volumes:
        - name: pgadmin-data
          persistentVolumeClaim:
            claimName: pgadmin4
      securityContext:
        fsGroup: 5050
        runAsGroup: 5050
        runAsUser: 5050
---
# Source: pgadmin4/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
spec:
  rules:
    - host: "anett-xmg-fusion-15-xfu15l19"
      http:
        paths:
          - path: /pgadmin4
            pathType: Prefix
            backend:
              service:
                name: pgadmin4
                port:
                  number: 80
---
# Source: pgadmin4/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "pgadmin4-test-connection"
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  securityContext:
    runAsNonRoot: true
    fsGroup: 5051
    runAsGroup: 5051
    runAsUser: 5051
  containers:
    - name: wget
      image: "docker.io/busybox:latest"
      env:
        - name: PGADMIN_HOST
          value: pgadmin4
        - name: PGADMIN_PORT
          value: "80"
      command:
        - /bin/sh
        - -ec
        - |
          response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)
          check=$(echo $response | grep -c '200 OK'); echo $check; if [[ $check -gt 0 ]]; then echo "Response OK"; else exit 1; fi
      resources:
        {}
      securityContext:
        readOnlyRootFilesystem: true
  restartPolicy: Never

---
# Source: timescale-config/templates/custom-secret-scripts.yaml
apiVersion: v1
kind: Secret
metadata:
  name: custom-secret-scripts
type: Opaque
data:
  timescale-secrets.sh: IyEvYmluL2Jhc2gKcHNxbCAtZCAiJDEiIC0tZmlsZT0tIC0tc2V0IE9OX0VSUk9SX1NUT1A9MSA8PCBfX1NRTF9fClNFVCBsb2dfc3RhdGVtZW50IFRPIG5vbmU7ICAgICAgLS0gcHJldmVudCB0aGVzZSBwYXNzd29yZHMgZnJvbSBiZWluZyBsb2dnZWQKQUxURVIgVVNFUiBoYWNrYXRob24gIFdJVEggUEFTU1dPUkQgJ2hhY2thdGhvbic7CkFMVEVSIFVTRVIgY2l0eWRhc2hib2FyZCAgV0lUSCBQQVNTV09SRCAnY2l0eWRhc2hib2FyZCc7Cl9fU1FMX18K
---
# Source: timescale-config/templates/custom-init-scripts.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-init-scripts
data:
  custom-config.sh: |-
    #!/bin/bash

    psql -d "$1" <<__SQL__
    CREATE ROLE hackathon WITH LOGIN;
    CREATE DATABASE hackathon OWNER hackathon;
    __SQL__

    psql -d "$1" <<__SQL__
    CREATE ROLE citydashboard WITH LOGIN;
    CREATE DATABASE citydashboard OWNER citydashboard;
    __SQL__

---
# Source: busybox/templates/pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - image: radial/busyboxplus:curl
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
    name: busybox
  #   volumeMounts:
  #   - mountPath: /
  #     name: logging-peristent-volume
  # volumes:
  #   - name: logging-peristent-volume
  #     persistentVolumeClaim:
  #       claimName: logging-volume-claim
  restartPolicy: Always

---
# Source: secrets/templates/ghcr-starwit-secret.yaml
apiVersion: v1
data:
  .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoid2l0Y2hwb3UiLCJwYXNzd29yZCI6ImdocF9CY3hPTXhYakgwSEhxRjdWeURNREhjSWtqdUNqYkgxNHNVdloiLCJhdXRoIjoiIn19fQ==
kind: Secret
metadata:
  creationTimestamp: null
  name: ghcr-starwit
type: kubernetes.io/dockerconfigjson

---
# Source: timescaledb-single/templates/serviceaccount-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: timescale-timescaledb
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: rbac
---
# Source: timescaledb-single/templates/configmap-patroni.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.---
apiVersion: v1
kind: ConfigMap
metadata:
  name: timescale-timescaledb-patroni
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: patroni
data:
  patroni.yaml: |
    bootstrap:
      dcs:
        loop_wait: 10
        maximum_lag_on_failover: 33554432
        postgresql:
          parameters:
            archive_command: /etc/timescaledb/scripts/pgbackrest_archive.sh %p
            archive_mode: "on"
            archive_timeout: 1800s
            autovacuum_analyze_scale_factor: 0.02
            autovacuum_max_workers: 10
            autovacuum_naptime: 5s
            autovacuum_vacuum_cost_limit: 500
            autovacuum_vacuum_scale_factor: 0.05
            hot_standby: "on"
            log_autovacuum_min_duration: 1min
            log_checkpoints: "on"
            log_connections: "on"
            log_disconnections: "on"
            log_line_prefix: '%t [%p]: [%c-%l] %u@%d,app=%a [%e] '
            log_lock_waits: "on"
            log_min_duration_statement: 1s
            log_statement: ddl
            max_connections: 100
            max_prepared_transactions: 150
            shared_preload_libraries: timescaledb,pg_stat_statements
            ssl: "on"
            ssl_cert_file: /etc/certificate/tls.crt
            ssl_key_file: /etc/certificate/tls.key
            tcp_keepalives_idle: 900
            tcp_keepalives_interval: 100
            temp_file_limit: 1GB
            timescaledb.passfile: ../.pgpass
            unix_socket_directories: /var/run/postgresql
            unix_socket_permissions: "0750"
            wal_level: hot_standby
            wal_log_hints: "on"
          use_pg_rewind: true
          use_slots: true
        retry_timeout: 10
        ttl: 30
      method: restore_or_initdb
      post_init: /etc/timescaledb/scripts/post_init.sh
      restore_or_initdb:
        command: |
          /etc/timescaledb/scripts/restore_or_initdb.sh --encoding=UTF8 --locale=C.UTF-8
        keep_existing_recovery_conf: true
    kubernetes:
      role_label: role
      scope_label: cluster-name
      use_endpoints: true
    log:
      level: WARNING
    postgresql:
      authentication:
        replication:
          username: standby
        superuser:
          username: postgres
      basebackup:
      - waldir: /var/lib/postgresql/wal/pg_wal
      callbacks:
        on_reload: /etc/timescaledb/scripts/patroni_callback.sh
        on_restart: /etc/timescaledb/scripts/patroni_callback.sh
        on_role_change: /etc/timescaledb/scripts/patroni_callback.sh
        on_start: /etc/timescaledb/scripts/patroni_callback.sh
        on_stop: /etc/timescaledb/scripts/patroni_callback.sh
      create_replica_methods:
      - pgbackrest
      - basebackup
      listen: 0.0.0.0:5432
      pg_hba:
      - local     all             postgres                              peer
      - local     all             all                                   password
      - hostnossl all,replication all                all                password
      - hostssl   all             all                127.0.0.1/32       password
      - hostssl   all             all                ::1/128            password
      - hostssl   replication     standby            all                password
      - hostssl   all             all                all                password
      pgbackrest:
        command: /etc/timescaledb/scripts/pgbackrest_restore.sh
        keep_data: true
        no_master: true
        no_params: true
      recovery_conf:
        restore_command: /etc/timescaledb/scripts/pgbackrest_archive_get.sh %f "%p"
      use_unix_socket: true
    restapi:
      listen: 0.0.0.0:8008
...
---
# Source: timescaledb-single/templates/configmap-pgbackrest.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: timescale-timescaledb-pgbackrest
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: pgbackrest
data:
  pgbackrest.conf: |
    [global]
    compress-level=3
    compress-type=lz4
    process-max=4
    repo1-cipher-type=none
    repo1-path=/cityos/timescale-timescaledb/
    repo1-retention-diff=2
    repo1-retention-full=2
    spool-path=/var/run/postgresql
    start-fast=y

    [poddb]
    pg1-port=5432
    pg1-host-user=postgres
    pg1-path=/var/lib/postgresql/data
    pg1-socket-path=/var/run/postgresql

    link-all=y

    [global:archive-push]

    [global:archive-get]
...
---
# Source: timescaledb-single/templates/configmap-scripts.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.---
apiVersion: v1
kind: ConfigMap
metadata:
  name: timescale-timescaledb-scripts
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: scripts
data:
  tstune.sh: |-
    #!/bin/sh
    
    set -eu
    
    # Exit if required variable is not set externally
    : "$TSTUNE_FILE"
    : "$WAL_VOLUME_SIZE"
    : "$DATA_VOLUME_SIZE"
    : "$RESOURCES_CPU_REQUESTS"
    : "$RESOURCES_MEMORY_REQUESTS"
    : "$RESOURCES_CPU_LIMIT"
    : "$RESOURCES_MEMORY_LIMIT"
    
    # Figure out how many cores are available
    CPUS="$RESOURCES_CPU_REQUESTS"
    if [ "$RESOURCES_CPU_REQUESTS" -eq 0 ]; then
        CPUS="${RESOURCES_CPU_LIMIT}"
    fi
    # Figure out how much memory is available
    MEMORY="$RESOURCES_MEMORY_REQUESTS"
    if [ "$RESOURCES_MEMORY_REQUESTS" -eq 0 ]; then
        MEMORY="${RESOURCES_MEMORY_LIMIT}"
    fi
    
    # Ensure tstune config file exists
    touch "${TSTUNE_FILE}"
    
    # Ensure tstune-generated config is included in postgresql.conf
    if [ -f "${PGDATA}/postgresql.base.conf" ] && ! grep "include_if_exists = '${TSTUNE_FILE}'" postgresql.base.conf -qxF; then
        echo "include_if_exists = '${TSTUNE_FILE}'" >> "${PGDATA}/postgresql.base.conf"
    fi
    
    # If there is a dedicated WAL Volume, we want to set max_wal_size to 60% of that volume
    # If there isn't a dedicated WAL Volume, we set it to 20% of the data volume
    if [ "${WAL_VOLUME_SIZE}" = "0" ]; then
        WALMAX="${DATA_VOLUME_SIZE}"
        WALPERCENT=20
    else
        WALMAX="${WAL_VOLUME_SIZE}"
        WALPERCENT=60
    fi
    
    WALMAX=$(numfmt --from=auto "${WALMAX}")
    
    # Wal segments are 16MB in size, in this way we get a "nice" number of the nearest
    # 16MB
    # walmax / 100 * walpercent / 16MB # below is a refactored with increased precision
    WALMAX=$(( WALMAX * WALPERCENT * 16 / 16777216 / 100  ))
    WALMIN=$(( WALMAX / 2 ))
    
    echo "max_wal_size=${WALMAX}MB" >> "${TSTUNE_FILE}"
    echo "min_wal_size=${WALMIN}MB" >> "${TSTUNE_FILE}"
    
    # Run tstune
    timescaledb-tune -quiet -conf-path "${TSTUNE_FILE}" -cpus "${CPUS}" -memory "${MEMORY}MB" -yes "$@"
    
  pgbackrest_archive.sh: |-
    #!/bin/sh
    
    # If no backup is configured, archive_command would normally fail. A failing archive_command on a cluster
    # is going to cause WAL to be kept around forever, meaning we'll fill up Volumes we have quite quickly.
    #
    # Therefore, if the backup is disabled, we always return exitcode 0 when archiving
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - archive - $1"
    }
    
    [ -z "$1" ] && log "Usage: $0 <WALFILE or DIRECTORY>" && exit 1
    
    : "${ENV_FILE:=${HOME}/.pgbackrest_environment}"
    if [ -f "${ENV_FILE}" ]; then
        echo "Sourcing ${ENV_FILE}"
        . "${ENV_FILE}"
    fi
    
    # PGBACKREST_BACKUP_ENABLED variable is passed in StatefulSet template
    [ "${PGBACKREST_BACKUP_ENABLED}" = "true" ] || exit 0
    
    exec pgbackrest --stanza=poddb archive-push "$@"
    
  pgbackrest_archive_get.sh: |-
    #!/bin/sh
    # PGBACKREST_BACKUP_ENABLED variable is passed in StatefulSet template
    [ "${PGBACKREST_BACKUP_ENABLED}" = "true" ] || exit 1
    
    : "${ENV_FILE:=${HOME}/.pgbackrest_environment}"
    if [ -f "${ENV_FILE}" ]; then
    echo "Sourcing ${ENV_FILE}"
    . "${ENV_FILE}"
    fi
    
    exec pgbackrest --stanza=poddb archive-get "${1}" "${2}"
    
  pgbackrest_bootstrap.sh: |-
    #!/bin/sh
    set -e
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - bootstrap - $1"
    }
    
    terminate() {
        log "Stopping"
        exit 1
    }
    # If we don't catch these signals, and we're still waiting for PostgreSQL
    # to be ready, we will not respond at all to a regular shutdown request,
    # therefore, we explicitly terminate if we receive these signals.
    trap terminate TERM QUIT
    
    while ! pg_isready -q; do
        log "Waiting for PostgreSQL to become available"
        sleep 3
    done
    
    # We'll be lazy; we wait for another while to allow the database to promote
    # to primary if it's the only one running
    sleep 10
    
    # If we are the primary, we want to create/validate the backup stanza
    if [ "$(psql -c "SELECT pg_is_in_recovery()::text" -AtXq)" = "false" ]; then
        pgbackrest check || {
            log "Creating pgBackrest stanza"
            pgbackrest --stanza=poddb stanza-create --log-level-stderr=info || exit 1
            log "Creating initial backup"
            pgbackrest --type=full backup || exit 1
        }
    fi
    
    log "Starting pgBackrest api to listen for backup requests"
    exec python3 /scripts/pgbackrest-rest.py --stanza=poddb --loglevel=debug
    
  pgbackrest_restore.sh: |
    #!/bin/sh
    # PGBACKREST_BACKUP_ENABLED variable is passed in StatefulSet template
    [ "${PGBACKREST_BACKUP_ENABLED}" = "true" ] || exit 1
    
    : "${ENV_FILE:=${HOME}/.pod_environment}"
    if [ -f "${ENV_FILE}" ]; then
    echo "Sourcing ${ENV_FILE}"
    . "${ENV_FILE}"
    fi
    
    # PGDATA and WALDIR are set in the StatefulSet template and are sourced from the ENV_FILE
    # PGDATA=
    # WALDIR=
    
    # A missing PGDATA points to Patroni removing a botched PGDATA, or manual
    # intervention. In this scenario, we need to recreate the DATA and WALDIRs
    # to keep pgBackRest happy
    [ -d "${PGDATA}" ] || install -o postgres -g postgres -d -m 0700 "${PGDATA}"
    [ -d "${WALDIR}" ] || install -o postgres -g postgres -d -m 0700 "${WALDIR}"
    
    exec pgbackrest --force --delta --log-level-console=detail restore
    
  restore_or_initdb.sh: |
    #!/bin/sh
    
    : "${ENV_FILE:=${HOME}/.pod_environment}"
    if [ -f "${ENV_FILE}" ]; then
        echo "Sourcing ${ENV_FILE}"
        . "${ENV_FILE}"
    fi
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - restore_or_initdb - $1"
    }
    
    # PGDATA and WALDIR are set in the StatefulSet template and are sourced from the ENV_FILE
    # PGDATA=
    # WALDIR=
    
    # A missing PGDATA points to Patroni removing a botched PGDATA, or manual
    # intervention. In this scenario, we need to recreate the DATA and WALDIRs
    # to keep pgBackRest happy
    [ -d "${PGDATA}" ] || install -o postgres -g postgres -d -m 0700 "${PGDATA}"
    [ -d "${WALDIR}" ] || install -o postgres -g postgres -d -m 0700 "${WALDIR}"
    
    if [ "${BOOTSTRAP_FROM_BACKUP}" = "1" ]; then
        log "Attempting restore from backup"
        # we want to override the environment with the environment
        # shellcheck disable=SC2046
        export $(env -i envdir /etc/pgbackrest/bootstrap env) > /dev/null
        # PGBACKREST_REPO1_PATH is set in the StatefulSet template and sourced from the ENV_FILE
    
        if [ -z "${PGBACKREST_REPO1_PATH}" ]; then
            log "Unconfigured repository path"
            cat << "__EOT__"
    
    TimescaleDB Single Helm Chart error:
    
    You should configure the bootstrapFromBackup in your Helm Chart section by explicitly setting
    the repo1-path to point to the backups.
    
    For more information, consult the admin guide:
    https://github.com/timescale/helm-charts/blob/main/charts/timescaledb-single/docs/admin-guide.md#bootstrap-from-backup
    
    __EOT__
    
            exit 1
        fi
    
        log "Listing available backup information"
        pgbackrest info
        EXITCODE=$?
        if [ ${EXITCODE} -ne 0 ]; then
            exit $EXITCODE
        fi
    
        pgbackrest --log-level-console=detail restore
        EXITCODE=$?
        if [ ${EXITCODE} -eq 0 ]; then
            log "pgBackRest restore finished succesfully, starting instance in recovery"
            # We want to ensure we do not overwrite a current backup repository with archives, therefore
            # we block archiving from succeeding until Patroni can takeover
            touch "${PGDATA}/recovery.signal"
            pg_ctl -D "${PGDATA}" start -o '--archive-command=/bin/false'
    
            while ! pg_isready -q; do
                log "Waiting for PostgreSQL to become available"
                sleep 3
            done
    
            # It is not trivial to figure out to what point we should restore, pgBackRest
            # should be fetching WAL segments until the WAL is exhausted. We'll ask pgBackRest
            # what the Maximum Wal is that it currently has; as soon as we see that, we can consider
            # the restore to be done
            while true; do
                MAX_BACKUP_WAL="$(pgbackrest info --output=json | python3 -c "import json,sys;obj=json.load(sys.stdin); print(obj[0]['archive'][0]['max']);")"
                log "Testing whether WAL file ${MAX_BACKUP_WAL} has been restored ..."
                [ -f "${PGDATA}/pg_wal/${MAX_BACKUP_WAL}" ] && break
                sleep 30;
            done
    
            # At this point we know the final WAL archive has been restored, we should be done.
            log "The WAL file ${MAX_BACKUP_WAL} has been successully restored, shutting down instance"
            pg_ctl -D "${PGDATA}" promote
            pg_ctl -D "${PGDATA}" stop -m fast
            log "Handing over control to Patroni ..."
        else
            log "Bootstrap from backup failed"
            exit 1
        fi
    else
        # Patroni attaches --scope and --datadir to the arguments, we need to strip them off as
        # initdb has no business with these parameters
        initdb_args=""
        for value in "$@"
        do
            case $value in
                "--scope"*)
                    ;;
                "--datadir"*)
                    ;;
                *)
                    initdb_args="${initdb_args} $value"
                    ;;
            esac
        done
    
        log "Invoking initdb"
        # shellcheck disable=SC2086
        initdb --auth-local=peer --auth-host=md5 --pgdata="${PGDATA}" --waldir="${WALDIR}" ${initdb_args}
    fi
    
    echo "include_if_exists = '${TSTUNE_FILE}'" >> "${PGDATA}/postgresql.conf"
    
  post_init.sh: |-
    #!/bin/sh
    : "${ENV_FILE:=${HOME}/.pod_environment}"
    if [ -f "${ENV_FILE}" ]; then
        echo "Sourcing ${ENV_FILE}"
        . "${ENV_FILE}"
    fi
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - post_init - $1"
    }
    
    log "Creating extension TimescaleDB in template1 and postgres databases"
    psql -d "$URL" <<__SQL__
        \connect template1
        -- As we're still only initializing, we cannot have synchronous_commit enabled just yet.
        SET synchronous_commit to 'off';
        CREATE EXTENSION timescaledb;
    
        \connect postgres
        SET synchronous_commit to 'off';
        CREATE EXTENSION timescaledb;
    __SQL__
    
    # POSTGRES_TABLESPACES is a comma-separated list of tablespaces to create
    # variable is passed in StatefulSet template
    : "${POSTGRES_TABLESPACES:=""}"
    for tablespace in $POSTGRES_TABLESPACES
    do
        log "Creating tablespace ${tablespace}"
        tablespacedir="${PGDATA}/tablespaces/${tablespace}/data"
        psql -d "$URL" --set tablespace="${tablespace}" --set directory="${tablespacedir}" --set ON_ERROR_STOP=1 <<__SQL__
        SET synchronous_commit to 'off';
        CREATE TABLESPACE :"tablespace" LOCATION :'directory';
    __SQL__
    done
    
    # This directory may contain user defined post init steps
    for file in /etc/timescaledb/post_init.d/*
    do
        [ -d "$file" ] && continue
        [ ! -r "$file" ] && continue
    
        case "$file" in
        *.sh)
            if [ -x "$file" ]; then
            log "Call post init script [ $file ]"
            "$file" "$@"
            EXITCODE=$?
            else
            log "Source post init script [ $file ]"
            . "$file"
            EXITCODE=$?
            fi
            ;;
        *.sql)
            log "Apply post init sql [ $file ]"
            # Disable synchronous_commit since we're initializing
            PGOPTIONS="-c synchronous_commit=local" psql -d "$URL" -f "$file"
            EXITCODE=$?
            ;;
        *.sql.gz)
            log "Decompress and apply post init sql [ $file ]"
            gunzip -c "$file" | PGOPTIONS="-c synchronous_commit=local" psql -d "$URL"
            EXITCODE=$?
            ;;
        *)
            log "Ignore unknown post init file type [ $file ]"
            EXITCODE=0
            ;;
        esac
        EXITCODE=$?
        if [ "$EXITCODE" != "0" ]
        then
            log "ERROR: post init script $file exited with exitcode $EXITCODE"
            exit $EXITCODE
        fi
    done
    
    # We exit 0 this script, otherwise the database initialization fails.
    exit 0
    
  patroni_callback.sh: |-
    #!/bin/sh
    set -e
    
    : "${ENV_FILE:=${HOME}/.pod_environment}"
    if [ -f "${ENV_FILE}" ]; then
        echo "Sourcing ${ENV_FILE}"
        . "${ENV_FILE}"
    fi
    
    for suffix in "$1" all
    do
        CALLBACK="/etc/timescaledb/callbacks/${suffix}"
        if [ -f "${CALLBACK}" ]
        then
        "${CALLBACK}" "$@"
        fi
    done
    
  lifecycle_preStop.sql: |-
    -- Doing a checkpoint (at the primary and the current instance) before starting
    -- the shutdown process will speed up the CHECKPOINT that is part of the shutdown
    -- process and the recovery after the pod is rescheduled.
    --
    -- We issue the CHECKPOINT at the primary always because:
    --
    -- > Restartpoints can't be performed more frequently than checkpoints in the
    -- > master because restartpoints can only be performed at checkpoint records.
    -- https://www.postgresql.org/docs/current/wal-configuration.html
    --
    -- While we're doing these preStop CHECKPOINTs we can still serve read/write
    -- queries to clients, whereas as soon as we initiate the shutdown, we terminate
    -- connections.
    --
    -- This therefore reduces downtime for the clients, at the cost of increasing (slightly)
    -- the time to stop the pod, and reducing write performance on the primary.
    --
    -- To further reduce downtime for clients, we will issue a switchover iff we are currently
    -- running as the primary. This again should be relatively fast, as we've just issued and
    -- waited for the CHECKPOINT to complete.
    --
    -- This is quite a lot of logic and work in a preStop command; however, if the preStop command
    -- fails for whatever reason, the normal Pod shutdown will commence, so it is only able to
    -- improve stuff without being able to break stuff.
    -- (The $(hostname) inside the switchover call safeguards that we never accidentally
    -- switchover the wrong primary).
    
    \pset pager off
    \set ON_ERROR_STOP true
    \set hostname `hostname`
    \set dsn_fmt 'user=postgres host=%s application_name=lifecycle:preStop@%s connect_timeout=5 options=''-c log_min_duration_statement=0'''
    
    SELECT
        pg_is_in_recovery() AS in_recovery,
        format(:'dsn_fmt', patroni_scope,                       :'hostname') AS primary_dsn,
        format(:'dsn_fmt', '/var/run/postgresql', :'hostname') AS local_dsn
    FROM
        current_setting('cluster_name') AS cs(patroni_scope)
    \gset
    
    \timing on
    \set ECHO queries
    
    -- There should be a CHECKPOINT at the primary
    \if :in_recovery
        \connect :"primary_dsn"
        CHECKPOINT;
    \endif
    
    -- There should also be a CHECKPOINT locally,
    -- for the primary, this may mean we do a double checkpoint,
    -- but the second one would be cheap anyway, so we leave that as is
    \connect :"local_dsn"
    SELECT 'Issuing checkpoint';
    CHECKPOINT;
    
    \if :in_recovery
        SELECT 'We are a replica: Successfully invoked checkpoints at the primary and locally.';
    \else
        SELECT 'We are a primary: Successfully invoked checkpoints, now issuing a switchover.';
        \! curl -s http://localhost:8008/switchover -XPOST -d '{"leader": "$(hostname)"}'
    \endif
    
...
---
# Source: timescaledb-single/templates/role-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: timescale-timescaledb
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: rbac
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  verbs:
  - create
  - get
  - list
  - patch
  - update
  - watch
  # delete is required only for 'patronictl remove'
  - delete
- apiGroups: [""]
  resources:
  - endpoints
  - endpoints/restricted
  verbs:
  - create
  - get
  - patch
  - update
  # the following three privileges are necessary only when using endpoints
  - list
  - watch
  # delete is required only for for 'patronictl remove'
  - delete
- apiGroups: [""]
  resources: ["pods"]
  verbs:
  - get
  - list
  - patch
  - update
  - watch
---
# Source: timescaledb-single/templates/rolebinding-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: timescale-timescaledb
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: rbac
subjects:
  - kind: ServiceAccount
    name: timescale-timescaledb
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: timescale-timescaledb
---
# Source: timescaledb-single/templates/svc-timescaledb-config.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: v1
kind: Service
metadata:
  name: timescale-timescaledb-config
  namespace: cityos
  labels:
    component: patroni
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: patroni
spec:
  selector:
    app: timescale-timescaledb
    cluster-name: timescale-timescaledb
  type: ClusterIP
  clusterIP: None
  ports:
  - name: patroni
    port: 8008
    protocol: TCP
---
# Source: timescaledb-single/templates/svc-timescaledb-replica.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: v1
kind: Service
metadata:
  name: timescale-timescaledb-replica
  namespace: cityos
  labels:
    component: postgres
    role: replica
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: postgres
spec:
  selector:
    app: timescale-timescaledb
    cluster-name: timescale-timescaledb
    role: replica
  type: ClusterIP
  ports:
  - name: postgresql
    # This always defaults to 5432
    port: 5432
    targetPort: postgresql
    protocol: TCP
---
# Source: timescaledb-single/templates/svc-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: v1
kind: Service
metadata:
  name: timescale-timescaledb
  namespace: cityos
  labels:
    role: master
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: timescaledb
spec:
  selector:
    app: timescale-timescaledb
    cluster-name: timescale-timescaledb
    role: master
  type: ClusterIP
  ports:
  - name: postgresql
    # This always defaults to 5432
    port: 5432
    targetPort: postgresql
    protocol: TCP
---
# Source: timescaledb-single/templates/statefulset-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: timescale-timescaledb
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: timescaledb
spec:
  serviceName: timescale-timescaledb
  replicas: 1
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: timescale-timescaledb
      release: timescale
  template:
    metadata:
      name: timescale-timescaledb
      labels:
        
        app: timescale-timescaledb
        chart: timescaledb-single-0.33.1
        release: timescale
        heritage: Helm
        cluster-name: timescale-timescaledb
        app.kubernetes.io/name: "timescale-timescaledb"
        app.kubernetes.io/version: 0.33.1
        app.kubernetes.io/component: timescaledb
    spec:
      serviceAccountName: timescale-timescaledb
      securityContext:
        # The postgres user inside the TimescaleDB image has uid=1000.
        # This configuration ensures the permissions of the mounts are suitable
        fsGroup: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
      initContainers:
      - name: tstune
        securityContext:
          allowPrivilegeEscalation: false
        image: "timescale/timescaledb-ha:pg14.6-ts2.9.1-p1"
        env:
        - name: TSTUNE_FILE
          value: /var/run/postgresql/timescaledb.conf
        - name: WAL_VOLUME_SIZE
          value: 1Gi
        - name: DATA_VOLUME_SIZE
          value: 2Gi
        - name: RESOURCES_CPU_REQUESTS
          valueFrom:
            resourceFieldRef:
              containerName: timescaledb
              resource: requests.cpu
              divisor: "1"
        - name: RESOURCES_MEMORY_REQUESTS
          valueFrom:
            resourceFieldRef:
              containerName: timescaledb
              resource: requests.memory
              divisor: 1Mi
        - name: RESOURCES_CPU_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: timescaledb
              resource: limits.cpu
              divisor: "1"
        - name: RESOURCES_MEMORY_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: timescaledb
              resource: limits.memory
              divisor: 1Mi
        # Command below will run the timescaledb-tune utility and configure min/max wal size based on PVCs size
        command:
          - sh
          - "-c"
          - '/etc/timescaledb/scripts/tstune.sh '
        volumeMounts:
        - name: socket-directory
          mountPath: /var/run/postgresql
        - name: timescaledb-scripts
          mountPath: /etc/timescaledb/scripts
          readOnly: true
        resources:
          
          {}
      # Issuing the final checkpoints on a busy database may take considerable time.
      # Unfinished checkpoints will require more time during startup, so the tradeoff
      # here is time spent in shutdown/time spent in startup.
      # We choose shutdown here, especially as during the largest part of the shutdown
      # we can still serve clients.
      terminationGracePeriodSeconds: 600
      containers:
      - name: timescaledb
        securityContext:
          allowPrivilegeEscalation: false
        image: "timescale/timescaledb-ha:pg14.6-ts2.9.1-p1"
        imagePullPolicy: Always
        lifecycle:
          preStop:
            exec:
              command:
              - psql
              - -X
              - --file
              - "/etc/timescaledb/scripts/lifecycle_preStop.sql"
        # When reusing an already existing volume it sometimes happens that the permissions
        # of the PGDATA and/or wal directory are incorrect. To guard against this, we always correctly
        # set the permissons of these directories before we hand over to Patroni.
        # We also create all the tablespaces that are defined, to ensure a smooth restore/recovery on a
        # pristine set of Volumes.
        # As PostgreSQL requires to have full control over the permissions of the tablespace directories,
        # we create a subdirectory "data" in every tablespace mountpoint. The full path of every tablespace
        # therefore always ends on "/data".
        # By creating a .pgpass file in the $HOME directory, we expose the superuser password
        # to processes that may not have it in their environment (like the preStop lifecycle hook).
        # To ensure Patroni will not mingle with this file, we give Patroni its own pgpass file.
        # As these files are in the $HOME directory, they are only available to *this* container,
        # and they are ephemeral.
        command:
          - /bin/bash
          - "-c"
          - |
            
            install -o postgres -g postgres -d -m 0700 "/var/lib/postgresql/data" "/var/lib/postgresql/wal/pg_wal" || exit 1
            TABLESPACES=""
            for tablespace in ; do
              install -o postgres -g postgres -d -m 0700 "/var/lib/postgresql/tablespaces/${tablespace}/data"
            done

            # Environment variables can be read by regular users of PostgreSQL. Especially in a Kubernetes
            # context it is likely that some secrets are part of those variables.
            # To ensure we expose as little as possible to the underlying PostgreSQL instance, we have a list
            # of allowed environment variable patterns to retain.
            #
            # We need the KUBERNETES_ environment variables for the native Kubernetes support of Patroni to work.
            #
            # NB: Patroni will remove all PATRONI_.* environment variables before starting PostgreSQL

            # We store the current environment, as initscripts, callbacks, archive_commands etc. may require
            # to have the environment available to them
            set -o posix
            export -p > "${HOME}/.pod_environment"
            export -p | grep PGBACKREST > "${HOME}/.pgbackrest_environment"

            for UNKNOWNVAR in $(env | awk -F '=' '!/^(PATRONI_.*|HOME|PGDATA|PGHOST|LC_.*|LANG|PATH|KUBERNETES_SERVICE_.*|AWS_ROLE_ARN|AWS_WEB_IDENTITY_TOKEN_FILE)=/ {print $1}')
            do
                unset "${UNKNOWNVAR}"
            done

            touch /var/run/postgresql/timescaledb.conf
            touch /var/run/postgresql/wal_status

            echo "*:*:*:postgres:${PATRONI_SUPERUSER_PASSWORD}" >> ${HOME}/.pgpass
            chmod 0600 ${HOME}/.pgpass

            export PATRONI_POSTGRESQL_PGPASS="${HOME}/.pgpass.patroni"

            exec patroni /etc/timescaledb/patroni.yaml
        env:
        # We use mixed case environment variables for Patroni User management,
        # as the variable themselves are documented to be PATRONI_<username>_OPTIONS.
        # Where possible, we want to have lowercase usernames in PostgreSQL as more complex postgres usernames
        # requiring quoting to be done in certain contexts, which many tools do not do correctly, or even at all.
        # https://patroni.readthedocs.io/en/latest/ENVIRONMENT.html#bootstrap-configuration
        - name: PATRONICTL_CONFIG_FILE
          value: "/etc/timescaledb/patroni.yaml"
        - name: PATRONI_admin_OPTIONS
          value: createrole,createdb
        - name: PATRONI_REPLICATION_USERNAME
          value: standby
        # To specify the PostgreSQL and Rest API connect addresses we need
        # the PATRONI_KUBERNETES_POD_IP to be available as a bash variable, so we can compose an
        # IP:PORT address later on
        - name: PATRONI_KUBERNETES_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PATRONI_POSTGRESQL_CONNECT_ADDRESS
          value: "$(PATRONI_KUBERNETES_POD_IP):5432"
        - name: PATRONI_RESTAPI_CONNECT_ADDRESS
          value: "$(PATRONI_KUBERNETES_POD_IP):8008"
        - name: PATRONI_KUBERNETES_PORTS
          value: '[{"name": "postgresql", "port": 5432}]'
        - name: PATRONI_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: PATRONI_POSTGRESQL_DATA_DIR
          value: "/var/lib/postgresql/data"
        - name: PATRONI_KUBERNETES_NAMESPACE
          value: cityos
        - name: PATRONI_KUBERNETES_LABELS
          value: "{app: timescale-timescaledb, cluster-name: timescale-timescaledb, release: timescale}"
        - name: PATRONI_SCOPE
          value: timescale-timescaledb
        - name: PGBACKREST_CONFIG
          value: /etc/pgbackrest/pgbackrest.conf
        # PGDATA and PGHOST are not required to let Patroni/PostgreSQL run correctly,
        # but for interactive sessions, callbacks and PostgreSQL tools they should be correct.
        - name: PGDATA
          value: "$(PATRONI_POSTGRESQL_DATA_DIR)"
        - name: PGHOST
          value: "/var/run/postgresql"
        - name: WALDIR
          value: "/var/lib/postgresql/wal/pg_wal"
        - name: BOOTSTRAP_FROM_BACKUP
          value: "0"
        - name: PGBACKREST_BACKUP_ENABLED
          value: "false"
        - name: TSTUNE_FILE
          value: /var/run/postgresql/timescaledb.conf
          # pgBackRest is also called using the archive_command if the backup is enabled.
          # this script will also need access to the environment variables specified for
          # the backup. This can be removed once we do not directly invoke pgBackRest
          # from inside the TimescaleDB container anymore
        envFrom:
        - secretRef:
            name: "timescale-timescaledb-credentials"
            optional: false
        - secretRef:
            name: "timescale-timescaledb-pgbackrest"
            optional: true
        ports:
        - containerPort: 8008
          name: patroni
        - containerPort: 5432
          name: postgresql
        readinessProbe:
          exec:
            command:
              - pg_isready
              - -h
              - /var/run/postgresql
          initialDelaySeconds: 5
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 6
        volumeMounts:
        - name: storage-volume
          mountPath: "/var/lib/postgresql"
          subPath: ""
        - name: wal-volume
          mountPath: "/var/lib/postgresql/wal"
          subPath: ""
        - mountPath: /etc/timescaledb/patroni.yaml
          subPath: patroni.yaml
          name: patroni-config
          readOnly: true
        - mountPath: /etc/timescaledb/scripts
          name: timescaledb-scripts
          readOnly: true
        - mountPath: /etc/pgbackrest_secrets
          name: pgbackrest-secrets
          readOnly: true
        - mountPath: "/etc/timescaledb/post_init.d"
          name: post-init
          readOnly: true
        - mountPath: /etc/certificate
          name: certificate
          readOnly: true
        - name: socket-directory
          mountPath: /var/run/postgresql
        
        - mountPath: /etc/pgbackrest
          name: pgbackrest
          readOnly: true
        - mountPath: /etc/pgbackrest/bootstrap
          name: pgbackrest-bootstrap
          readOnly: true
        resources:
          
          {}
      volumes:
      - name: socket-directory
        emptyDir: {}
      - name: patroni-config
        configMap:
          name: timescale-timescaledb-patroni
      - name: timescaledb-scripts
        configMap:
          name: timescale-timescaledb-scripts
          defaultMode: 488 # 0750 permissions
      
      - name: post-init
        projected:
          defaultMode: 0750
          sources:
            
            - configMap:
                name: custom-init-scripts
                optional: true
            - secret:
                name: custom-secret-scripts
                optional: true
      - name: pgbouncer
        configMap:
          name: timescale-timescaledb-pgbouncer
          defaultMode: 416 # 0640 permissions
          optional: true
      - name: pgbackrest
        configMap:
          name: timescale-timescaledb-pgbackrest
          defaultMode: 416 # 0640 permissions
          optional: true
      - name: pgbackrest-secrets
        secret:
          secretName: timescale-timescaledb-pgbackrest-secrets
          defaultMode: 416
          optional: true
      - name: certificate
        secret:
          secretName: "timescale-timescaledb-certificate"
          defaultMode: 416 # 0640 permissions
      - name: pgbackrest-bootstrap
        secret:
          secretName: pgbackrest-bootstrap
          optional: True
  volumeClaimTemplates:
    - metadata:
        name: storage-volume
        annotations:
        labels:
          app: timescale-timescaledb
          release: timescale
          heritage: Helm
          cluster-name: timescale-timescaledb
          purpose: data-directory
      spec:
        accessModes:
          
        - ReadWriteOnce
        resources:
          requests:
            storage: "2Gi"
    - metadata:
        name: wal-volume
        annotations:
        labels:
          app: timescale-timescaledb
          release: timescale
          heritage: Helm
          cluster-name: timescale-timescaledb
          purpose: wal-directory
      spec:
        accessModes:
          
        - ReadWriteOnce
        resources:
          requests:
            storage: "1Gi"
---
# Source: timescaledb-single/templates/configmap-pgbackrest.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
---
# Source: timescaledb-single/templates/configmap-pgbouncer.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
---
# Source: timescaledb-single/templates/pgbackrest.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
---
# Source: timescaledb-single/templates/secret-certificate.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: v1
kind: Secret
metadata:
  name: "timescale-timescaledb-certificate"
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: certificates
  annotations:
    "helm.sh/hook": pre-install,post-delete
    "helm.sh/hook-weight": "0"
type: kubernetes.io/tls
stringData:
  tls.crt: "-----BEGIN CERTIFICATE-----\nMIIDLDCCAhSgAwIBAgIRAL6y9EB9z/ItQP7H3fdJD78wDQYJKoZIhvcNAQELBQAw\nIDEeMBwGA1UEAxMVdGltZXNjYWxlLXRpbWVzY2FsZWRiMB4XDTIzMDgxMzE4MjAy\nM1oXDTI4MDgxMjE4MjAyM1owIDEeMBwGA1UEAxMVdGltZXNjYWxlLXRpbWVzY2Fs\nZWRiMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA0wWCNyugFqyarzxU\nyQmNg3/4GJ6BqoQNJll7BemFWy2FD4qv3zC6xRBpgEbq4GNtQrXAmG4/ZMW1zVEp\n1sJyM8g11JbHU62j1AP+XaikzF3CqoKwW4uZ9QOYgJoVhS7lR7szrEQfCaLcJWEl\nWOnLcHOWm0jj7R4+m+iCLbiCq5/l5xtzGVs3sdnE7bFhp/BwRM6Hg40jXJxZ1bdU\nJIwyDTMHQeX1L5cETbS/ygsHafVh04ktjJynyWEuliIDOabLnHeogW8Ft5OjgeZB\nzhSz5ZLpNBdY+gWmoj4LVT/oXMOW19P6iWUbXrwZZ7QvZyA2kaCCiewfNl5bwRlm\nF23/uwIDAQABo2EwXzAOBgNVHQ8BAf8EBAMCAqQwHQYDVR0lBBYwFAYIKwYBBQUH\nAwEGCCsGAQUFBwMCMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYEFL2RNe1tBh2p\nH8Bmnd37fY/Z+/jyMA0GCSqGSIb3DQEBCwUAA4IBAQBU04+HuAuWRY5+GsZoECL0\n0aPCJeKAwv1MmczHjBNtyUlQBmv05Xo4m+SAO/fIEDbGw4+aGJSSkEZSoKBfYdFA\n6jYkpDb4rDj3tkA8syZezK/YBg1BRyPmWhTfCxqbP1CNdJSKJjqgMbBaW+K9ts60\nGk2d4HAF3npxYiA0/bzmAgX6RaqgNjL+NmlPfJIl/kAq0Wal3XhOQqor4n0wj0jT\n7+ft0L9j2nZuNXFiEurN1hAb0fI3XaYqOCtjVdCptVGGOP44WhXNayjDF4Y4r8SA\nRGIBSdKoqg6plhKvw7Fk85iXcrVMndvcInoQuDfUjtsLLz1EVQOolHIvD4ag4T8i\n-----END CERTIFICATE-----\n"
  tls.key: "-----BEGIN RSA PRIVATE KEY-----\nMIIEpAIBAAKCAQEA0wWCNyugFqyarzxUyQmNg3/4GJ6BqoQNJll7BemFWy2FD4qv\n3zC6xRBpgEbq4GNtQrXAmG4/ZMW1zVEp1sJyM8g11JbHU62j1AP+XaikzF3CqoKw\nW4uZ9QOYgJoVhS7lR7szrEQfCaLcJWElWOnLcHOWm0jj7R4+m+iCLbiCq5/l5xtz\nGVs3sdnE7bFhp/BwRM6Hg40jXJxZ1bdUJIwyDTMHQeX1L5cETbS/ygsHafVh04kt\njJynyWEuliIDOabLnHeogW8Ft5OjgeZBzhSz5ZLpNBdY+gWmoj4LVT/oXMOW19P6\niWUbXrwZZ7QvZyA2kaCCiewfNl5bwRlmF23/uwIDAQABAoIBAHsoRuQ+INO67xiA\n5JbkbWQnBrwPnmvPYAFWzfcYYE1jlroIXjarsQWcW4aTXTeSr8z0WLBd/7+nqEBp\nMzr1B0PzgH9PdIwA1pHBtu4go/XmbtunJJh5NGSMMbA0vYSaZxRjTHgo1diA5W0n\ncm2fHa3GuEpu0x+rvCywGXHnstPIkkq2TnMz3xlTDp+Xszo/lJ/zDMMhTbzC3j2w\nY/AzXvs0eDm1fUntxPXsHIcPWcmA/S/yovP1JA29AgV941lz2oclgcYR3rQZIFGM\nyKYdFDWlHIBVOwbcrAkybilgxaYzqInSy1t98+kKT+txOGOEOXsr1fBNtBCUdds0\nvFNHMqkCgYEA6vlaqJIAbPvI+nSSTEj+mJUI00W/U8pZkG1+QMqId3I3r77GyuBw\nSvNyOki9ZAKYjPHsaNzfbh1Py8+Mu4FtGoR7fx56jXgjP1lGNDlekqsORlAGerL0\nNvMdZXPHWuH5cvITDayErcBaqmJ/p5ABMtQbc+zYak6O5sOi2U30sF0CgYEA5ed2\n4SCmDWzxlPVXT/0HERvfdDOkwBAxKgKLq2Yev06Eo4zNPIgI2OfGnjHCCXq9hT+i\n+S6qEC0rIqdQaQ/SYQTQ6YE8hW7mbsF/vqNIrV/2lVkWfOu7H8womMpha+hmCPR5\n2DMbZbwAlGpqY+TguRc+aXFyozNe5DWsaxFRzvcCgYEAlc+VdZRgoRuG7+gSvoBr\nkmyyy8DUU1/X6rVkoXp/t2gjCPHhXMfr0F2WnoJx/amKCZLU36ay7UnS7mbNm17R\naO6+rjWIfVnzXxhUIkfRfEGDQGEVQHWILuCiBWTj3ZXlrNuaOkWb8guJwsg6C2NM\n4FNLjYWcFWswwQBKe7GwKCECgYB6alUSaqAAZhH8juZ+4OvyYDULWrRO0FBdXh2g\nli+pDdXyf/aIVKbhWAiaE0SOZ5yiZtAQfphvJJ3jRdO7HYGXx0i2DOp84iAwuTeb\n+rWoG4Dg5R7kRWDN/ZdgQ+K74nLyMbrvM8dS1Fg6rq8XobdG1umUm6O+4ouUWEAG\ngqJc4QKBgQChKRCkJzBJNd1mNtwd+iBJ1KSDrYQYIjnH3BTujKRt8DfQDewiDZGm\nSxvpyAHFh1KD9uYmQpkMqiOXK1uJHTYxSGe1+zeM1qr7c+BhX7S03ROn40T/oPDV\nuNFtsEJz11h9JqtdZ9xZdvS1N1f7X6T4iepyyCyBoGV69Q8bQ2apDw==\n-----END RSA PRIVATE KEY-----\n"
...
---
# Source: timescaledb-single/templates/secret-patroni.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: v1
kind: Secret
metadata:
  name: "timescale-timescaledb-credentials"
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: patroni
  annotations:
    "helm.sh/hook": pre-install,post-delete
    "helm.sh/hook-weight": "0"
    "helm.sh/resource-policy": keep
type: Opaque
stringData:
  PATRONI_SUPERUSER_PASSWORD: "1CiGr1d8y4Emyd4s"
  PATRONI_REPLICATION_PASSWORD: "UrkOnFYP9MJ1ZtAa"
  PATRONI_admin_PASSWORD: "vHxYhuwgFpF9G4jA"
...
---
# Source: timescaledb-single/templates/secret-pgbackrest.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: v1
kind: Secret
metadata:
  name: "timescale-timescaledb-pgbackrest"
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: pgbackrest
  annotations:
    "helm.sh/hook": pre-install,post-delete
    "helm.sh/hook-weight": "0"
    "helm.sh/resource-policy": keep
type: Opaque
stringData:
  {}
...
---
# Source: timescaledb-single/templates/job-update-patroni.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: batch/v1
kind: Job
metadata:
  name: "timescale-timescaledb-patroni-1"
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: patroni
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  activeDeadlineSeconds: 120
  template:
    metadata:
      labels:
        
        app: timescale-timescaledb
        chart: timescaledb-single-0.33.1
        release: timescale
        heritage: Helm
        cluster-name: timescale-timescaledb
        app.kubernetes.io/name: "timescale-timescaledb"
        app.kubernetes.io/version: 0.33.1
    spec:
      restartPolicy: OnFailure
      containers:
      - name: timescale-timescaledb-patch-patroni-config
        image: "curlimages/curl:7.87.0"
        imagePullPolicy: Always
        command: ["/bin/sh"]
        # Patching the Patroni configuration is good, however it should not block an upgrade from going through
        # Therefore we ensure we always exit with an exitcode 0, so that Helm is satisfied with this upgrade job
        args:
        - '-c'
        - |
          /usr/bin/curl --connect-timeout 30 --include --request PATCH --data \
          "{\"loop_wait\":10,\"maximum_lag_on_failover\":33554432,\"postgresql\":{\"parameters\":{\"archive_command\":\"/etc/timescaledb/scripts/pgbackrest_archive.sh %p\",\"archive_mode\":\"on\",\"archive_timeout\":\"1800s\",\"autovacuum_analyze_scale_factor\":0.02,\"autovacuum_max_workers\":10,\"autovacuum_naptime\":\"5s\",\"autovacuum_vacuum_cost_limit\":500,\"autovacuum_vacuum_scale_factor\":0.05,\"hot_standby\":\"on\",\"log_autovacuum_min_duration\":\"1min\",\"log_checkpoints\":\"on\",\"log_connections\":\"on\",\"log_disconnections\":\"on\",\"log_line_prefix\":\"%t [%p]: [%c-%l] %u@%d,app=%a [%e] \",\"log_lock_waits\":\"on\",\"log_min_duration_statement\":\"1s\",\"log_statement\":\"ddl\",\"max_connections\":100,\"max_prepared_transactions\":150,\"shared_preload_libraries\":\"timescaledb,pg_stat_statements\",\"ssl\":\"on\",\"ssl_cert_file\":\"/etc/certificate/tls.crt\",\"ssl_key_file\":\"/etc/certificate/tls.key\",\"tcp_keepalives_idle\":900,\"tcp_keepalives_interval\":100,\"temp_file_limit\":\"1GB\",\"timescaledb.passfile\":\"../.pgpass\",\"unix_socket_directories\":\"/var/run/postgresql\",\"unix_socket_permissions\":\"0750\",\"wal_level\":\"hot_standby\",\"wal_log_hints\":\"on\"},\"use_pg_rewind\":true,\"use_slots\":true},\"retry_timeout\":10,\"ttl\":30}" \
          "http://timescale-timescaledb-config:8008/config"
          exit 0

---
# Source: pgadmin4/templates/networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
spec:
  policyTypes:
    - Ingress
  podSelector:
    matchLabels:
      app.kubernetes.io/name: pgadmin4
      app.kubernetes.io/instance: pgadmin4
  ingress:
  - ports:
    - port: 80
---
# Source: pgadmin4/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  password: "U3VwZXJTZWNyZXQ="
---
# Source: pgadmin4/templates/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
# Source: pgadmin4/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: pgadmin4
    app.kubernetes.io/instance: pgadmin4
---
# Source: pgadmin4/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: pgadmin4
      app.kubernetes.io/instance: pgadmin4
  template:
    metadata:
      labels:
        app.kubernetes.io/name: pgadmin4
        app.kubernetes.io/instance: pgadmin4
      annotations:
        checksum/secret: 2acaa91f3a0987d847c541fcd7622f6419a80506088a14f3e4821dbb16fd3a9f

    spec:
      containers:
        - name: pgadmin4
          image: "docker.io/dpage/pgadmin4:7.4"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              port: http
              path: "/pgadmin4/misc/ping"
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 15
          readinessProbe:
            httpGet:
              port: http
              path: "/pgadmin4/misc/ping"
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 15
          env:
            - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION
              value: "False"
            - name: PGADMIN_DEFAULT_EMAIL
              value: chart@domain.com
            - name: PGADMIN_DEFAULT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: pgadmin4
                  key: password
            - name: SCRIPT_NAME
              value: /pgadmin4
          volumeMounts:
            - name: pgadmin-data
              mountPath: /var/lib/pgadmin
          resources:
            {}
      volumes:
        - name: pgadmin-data
          persistentVolumeClaim:
            claimName: pgadmin4
      securityContext:
        fsGroup: 5050
        runAsGroup: 5050
        runAsUser: 5050
---
# Source: pgadmin4/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
spec:
  rules:
    - host: "anett-xmg-fusion-15-xfu15l19"
      http:
        paths:
          - path: /pgadmin4
            pathType: Prefix
            backend:
              service:
                name: pgadmin4
                port:
                  number: 80
---
# Source: pgadmin4/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "pgadmin4-test-connection"
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  securityContext:
    runAsNonRoot: true
    fsGroup: 5051
    runAsGroup: 5051
    runAsUser: 5051
  containers:
    - name: wget
      image: "docker.io/busybox:latest"
      env:
        - name: PGADMIN_HOST
          value: pgadmin4
        - name: PGADMIN_PORT
          value: "80"
      command:
        - /bin/sh
        - -ec
        - |
          response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)
          check=$(echo $response | grep -c '200 OK'); echo $check; if [[ $check -gt 0 ]]; then echo "Response OK"; else exit 1; fi
      resources:
        {}
      securityContext:
        readOnlyRootFilesystem: true
  restartPolicy: Never

---
# Source: timescale-config/templates/custom-secret-scripts.yaml
apiVersion: v1
kind: Secret
metadata:
  name: custom-secret-scripts
type: Opaque
data:
  timescale-secrets.sh: IyEvYmluL2Jhc2gKcHNxbCAtZCAiJDEiIC0tZmlsZT0tIC0tc2V0IE9OX0VSUk9SX1NUT1A9MSA8PCBfX1NRTF9fClNFVCBsb2dfc3RhdGVtZW50IFRPIG5vbmU7ICAgICAgLS0gcHJldmVudCB0aGVzZSBwYXNzd29yZHMgZnJvbSBiZWluZyBsb2dnZWQKQUxURVIgVVNFUiBoYWNrYXRob24gIFdJVEggUEFTU1dPUkQgJ2hhY2thdGhvbic7CkFMVEVSIFVTRVIgY2l0eWRhc2hib2FyZCAgV0lUSCBQQVNTV09SRCAnY2l0eWRhc2hib2FyZCc7Cl9fU1FMX18K
---
# Source: timescale-config/templates/custom-init-scripts.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-init-scripts
data:
  custom-config.sh: |-
    #!/bin/bash

    psql -d "$1" <<__SQL__
    CREATE ROLE hackathon WITH LOGIN;
    CREATE DATABASE hackathon OWNER hackathon;
    __SQL__

    psql -d "$1" <<__SQL__
    CREATE ROLE citydashboard WITH LOGIN;
    CREATE DATABASE citydashboard OWNER citydashboard;
    __SQL__

---
# Source: busybox/templates/pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - image: radial/busyboxplus:curl
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
    name: busybox
  #   volumeMounts:
  #   - mountPath: /
  #     name: logging-peristent-volume
  # volumes:
  #   - name: logging-peristent-volume
  #     persistentVolumeClaim:
  #       claimName: logging-volume-claim
  restartPolicy: Always

---
# Source: secrets/templates/ghcr-starwit-secret.yaml
apiVersion: v1
data:
  .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoid2l0Y2hwb3UiLCJwYXNzd29yZCI6ImdocF9CY3hPTXhYakgwSEhxRjdWeURNREhjSWtqdUNqYkgxNHNVdloiLCJhdXRoIjoiIn19fQ==
kind: Secret
metadata:
  creationTimestamp: null
  name: ghcr-starwit
type: kubernetes.io/dockerconfigjson

---
# Source: timescaledb-single/templates/serviceaccount-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: timescale-timescaledb
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: rbac
---
# Source: timescaledb-single/templates/configmap-patroni.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.---
apiVersion: v1
kind: ConfigMap
metadata:
  name: timescale-timescaledb-patroni
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: patroni
data:
  patroni.yaml: |
    bootstrap:
      dcs:
        loop_wait: 10
        maximum_lag_on_failover: 33554432
        postgresql:
          parameters:
            archive_command: /etc/timescaledb/scripts/pgbackrest_archive.sh %p
            archive_mode: "on"
            archive_timeout: 1800s
            autovacuum_analyze_scale_factor: 0.02
            autovacuum_max_workers: 10
            autovacuum_naptime: 5s
            autovacuum_vacuum_cost_limit: 500
            autovacuum_vacuum_scale_factor: 0.05
            hot_standby: "on"
            log_autovacuum_min_duration: 1min
            log_checkpoints: "on"
            log_connections: "on"
            log_disconnections: "on"
            log_line_prefix: '%t [%p]: [%c-%l] %u@%d,app=%a [%e] '
            log_lock_waits: "on"
            log_min_duration_statement: 1s
            log_statement: ddl
            max_connections: 100
            max_prepared_transactions: 150
            shared_preload_libraries: timescaledb,pg_stat_statements
            ssl: "on"
            ssl_cert_file: /etc/certificate/tls.crt
            ssl_key_file: /etc/certificate/tls.key
            tcp_keepalives_idle: 900
            tcp_keepalives_interval: 100
            temp_file_limit: 1GB
            timescaledb.passfile: ../.pgpass
            unix_socket_directories: /var/run/postgresql
            unix_socket_permissions: "0750"
            wal_level: hot_standby
            wal_log_hints: "on"
          use_pg_rewind: true
          use_slots: true
        retry_timeout: 10
        ttl: 30
      method: restore_or_initdb
      post_init: /etc/timescaledb/scripts/post_init.sh
      restore_or_initdb:
        command: |
          /etc/timescaledb/scripts/restore_or_initdb.sh --encoding=UTF8 --locale=C.UTF-8
        keep_existing_recovery_conf: true
    kubernetes:
      role_label: role
      scope_label: cluster-name
      use_endpoints: true
    log:
      level: WARNING
    postgresql:
      authentication:
        replication:
          username: standby
        superuser:
          username: postgres
      basebackup:
      - waldir: /var/lib/postgresql/wal/pg_wal
      callbacks:
        on_reload: /etc/timescaledb/scripts/patroni_callback.sh
        on_restart: /etc/timescaledb/scripts/patroni_callback.sh
        on_role_change: /etc/timescaledb/scripts/patroni_callback.sh
        on_start: /etc/timescaledb/scripts/patroni_callback.sh
        on_stop: /etc/timescaledb/scripts/patroni_callback.sh
      create_replica_methods:
      - pgbackrest
      - basebackup
      listen: 0.0.0.0:5432
      pg_hba:
      - local     all             postgres                              peer
      - local     all             all                                   password
      - hostnossl all,replication all                all                password
      - hostssl   all             all                127.0.0.1/32       password
      - hostssl   all             all                ::1/128            password
      - hostssl   replication     standby            all                password
      - hostssl   all             all                all                password
      pgbackrest:
        command: /etc/timescaledb/scripts/pgbackrest_restore.sh
        keep_data: true
        no_master: true
        no_params: true
      recovery_conf:
        restore_command: /etc/timescaledb/scripts/pgbackrest_archive_get.sh %f "%p"
      use_unix_socket: true
    restapi:
      listen: 0.0.0.0:8008
...
---
# Source: timescaledb-single/templates/configmap-pgbackrest.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: timescale-timescaledb-pgbackrest
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: pgbackrest
data:
  pgbackrest.conf: |
    [global]
    compress-level=3
    compress-type=lz4
    process-max=4
    repo1-cipher-type=none
    repo1-path=/cityos/timescale-timescaledb/
    repo1-retention-diff=2
    repo1-retention-full=2
    spool-path=/var/run/postgresql
    start-fast=y

    [poddb]
    pg1-port=5432
    pg1-host-user=postgres
    pg1-path=/var/lib/postgresql/data
    pg1-socket-path=/var/run/postgresql

    link-all=y

    [global:archive-push]

    [global:archive-get]
...
---
# Source: timescaledb-single/templates/configmap-scripts.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.---
apiVersion: v1
kind: ConfigMap
metadata:
  name: timescale-timescaledb-scripts
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: scripts
data:
  tstune.sh: |-
    #!/bin/sh
    
    set -eu
    
    # Exit if required variable is not set externally
    : "$TSTUNE_FILE"
    : "$WAL_VOLUME_SIZE"
    : "$DATA_VOLUME_SIZE"
    : "$RESOURCES_CPU_REQUESTS"
    : "$RESOURCES_MEMORY_REQUESTS"
    : "$RESOURCES_CPU_LIMIT"
    : "$RESOURCES_MEMORY_LIMIT"
    
    # Figure out how many cores are available
    CPUS="$RESOURCES_CPU_REQUESTS"
    if [ "$RESOURCES_CPU_REQUESTS" -eq 0 ]; then
        CPUS="${RESOURCES_CPU_LIMIT}"
    fi
    # Figure out how much memory is available
    MEMORY="$RESOURCES_MEMORY_REQUESTS"
    if [ "$RESOURCES_MEMORY_REQUESTS" -eq 0 ]; then
        MEMORY="${RESOURCES_MEMORY_LIMIT}"
    fi
    
    # Ensure tstune config file exists
    touch "${TSTUNE_FILE}"
    
    # Ensure tstune-generated config is included in postgresql.conf
    if [ -f "${PGDATA}/postgresql.base.conf" ] && ! grep "include_if_exists = '${TSTUNE_FILE}'" postgresql.base.conf -qxF; then
        echo "include_if_exists = '${TSTUNE_FILE}'" >> "${PGDATA}/postgresql.base.conf"
    fi
    
    # If there is a dedicated WAL Volume, we want to set max_wal_size to 60% of that volume
    # If there isn't a dedicated WAL Volume, we set it to 20% of the data volume
    if [ "${WAL_VOLUME_SIZE}" = "0" ]; then
        WALMAX="${DATA_VOLUME_SIZE}"
        WALPERCENT=20
    else
        WALMAX="${WAL_VOLUME_SIZE}"
        WALPERCENT=60
    fi
    
    WALMAX=$(numfmt --from=auto "${WALMAX}")
    
    # Wal segments are 16MB in size, in this way we get a "nice" number of the nearest
    # 16MB
    # walmax / 100 * walpercent / 16MB # below is a refactored with increased precision
    WALMAX=$(( WALMAX * WALPERCENT * 16 / 16777216 / 100  ))
    WALMIN=$(( WALMAX / 2 ))
    
    echo "max_wal_size=${WALMAX}MB" >> "${TSTUNE_FILE}"
    echo "min_wal_size=${WALMIN}MB" >> "${TSTUNE_FILE}"
    
    # Run tstune
    timescaledb-tune -quiet -conf-path "${TSTUNE_FILE}" -cpus "${CPUS}" -memory "${MEMORY}MB" -yes "$@"
    
  pgbackrest_archive.sh: |-
    #!/bin/sh
    
    # If no backup is configured, archive_command would normally fail. A failing archive_command on a cluster
    # is going to cause WAL to be kept around forever, meaning we'll fill up Volumes we have quite quickly.
    #
    # Therefore, if the backup is disabled, we always return exitcode 0 when archiving
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - archive - $1"
    }
    
    [ -z "$1" ] && log "Usage: $0 <WALFILE or DIRECTORY>" && exit 1
    
    : "${ENV_FILE:=${HOME}/.pgbackrest_environment}"
    if [ -f "${ENV_FILE}" ]; then
        echo "Sourcing ${ENV_FILE}"
        . "${ENV_FILE}"
    fi
    
    # PGBACKREST_BACKUP_ENABLED variable is passed in StatefulSet template
    [ "${PGBACKREST_BACKUP_ENABLED}" = "true" ] || exit 0
    
    exec pgbackrest --stanza=poddb archive-push "$@"
    
  pgbackrest_archive_get.sh: |-
    #!/bin/sh
    # PGBACKREST_BACKUP_ENABLED variable is passed in StatefulSet template
    [ "${PGBACKREST_BACKUP_ENABLED}" = "true" ] || exit 1
    
    : "${ENV_FILE:=${HOME}/.pgbackrest_environment}"
    if [ -f "${ENV_FILE}" ]; then
    echo "Sourcing ${ENV_FILE}"
    . "${ENV_FILE}"
    fi
    
    exec pgbackrest --stanza=poddb archive-get "${1}" "${2}"
    
  pgbackrest_bootstrap.sh: |-
    #!/bin/sh
    set -e
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - bootstrap - $1"
    }
    
    terminate() {
        log "Stopping"
        exit 1
    }
    # If we don't catch these signals, and we're still waiting for PostgreSQL
    # to be ready, we will not respond at all to a regular shutdown request,
    # therefore, we explicitly terminate if we receive these signals.
    trap terminate TERM QUIT
    
    while ! pg_isready -q; do
        log "Waiting for PostgreSQL to become available"
        sleep 3
    done
    
    # We'll be lazy; we wait for another while to allow the database to promote
    # to primary if it's the only one running
    sleep 10
    
    # If we are the primary, we want to create/validate the backup stanza
    if [ "$(psql -c "SELECT pg_is_in_recovery()::text" -AtXq)" = "false" ]; then
        pgbackrest check || {
            log "Creating pgBackrest stanza"
            pgbackrest --stanza=poddb stanza-create --log-level-stderr=info || exit 1
            log "Creating initial backup"
            pgbackrest --type=full backup || exit 1
        }
    fi
    
    log "Starting pgBackrest api to listen for backup requests"
    exec python3 /scripts/pgbackrest-rest.py --stanza=poddb --loglevel=debug
    
  pgbackrest_restore.sh: |
    #!/bin/sh
    # PGBACKREST_BACKUP_ENABLED variable is passed in StatefulSet template
    [ "${PGBACKREST_BACKUP_ENABLED}" = "true" ] || exit 1
    
    : "${ENV_FILE:=${HOME}/.pod_environment}"
    if [ -f "${ENV_FILE}" ]; then
    echo "Sourcing ${ENV_FILE}"
    . "${ENV_FILE}"
    fi
    
    # PGDATA and WALDIR are set in the StatefulSet template and are sourced from the ENV_FILE
    # PGDATA=
    # WALDIR=
    
    # A missing PGDATA points to Patroni removing a botched PGDATA, or manual
    # intervention. In this scenario, we need to recreate the DATA and WALDIRs
    # to keep pgBackRest happy
    [ -d "${PGDATA}" ] || install -o postgres -g postgres -d -m 0700 "${PGDATA}"
    [ -d "${WALDIR}" ] || install -o postgres -g postgres -d -m 0700 "${WALDIR}"
    
    exec pgbackrest --force --delta --log-level-console=detail restore
    
  restore_or_initdb.sh: |
    #!/bin/sh
    
    : "${ENV_FILE:=${HOME}/.pod_environment}"
    if [ -f "${ENV_FILE}" ]; then
        echo "Sourcing ${ENV_FILE}"
        . "${ENV_FILE}"
    fi
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - restore_or_initdb - $1"
    }
    
    # PGDATA and WALDIR are set in the StatefulSet template and are sourced from the ENV_FILE
    # PGDATA=
    # WALDIR=
    
    # A missing PGDATA points to Patroni removing a botched PGDATA, or manual
    # intervention. In this scenario, we need to recreate the DATA and WALDIRs
    # to keep pgBackRest happy
    [ -d "${PGDATA}" ] || install -o postgres -g postgres -d -m 0700 "${PGDATA}"
    [ -d "${WALDIR}" ] || install -o postgres -g postgres -d -m 0700 "${WALDIR}"
    
    if [ "${BOOTSTRAP_FROM_BACKUP}" = "1" ]; then
        log "Attempting restore from backup"
        # we want to override the environment with the environment
        # shellcheck disable=SC2046
        export $(env -i envdir /etc/pgbackrest/bootstrap env) > /dev/null
        # PGBACKREST_REPO1_PATH is set in the StatefulSet template and sourced from the ENV_FILE
    
        if [ -z "${PGBACKREST_REPO1_PATH}" ]; then
            log "Unconfigured repository path"
            cat << "__EOT__"
    
    TimescaleDB Single Helm Chart error:
    
    You should configure the bootstrapFromBackup in your Helm Chart section by explicitly setting
    the repo1-path to point to the backups.
    
    For more information, consult the admin guide:
    https://github.com/timescale/helm-charts/blob/main/charts/timescaledb-single/docs/admin-guide.md#bootstrap-from-backup
    
    __EOT__
    
            exit 1
        fi
    
        log "Listing available backup information"
        pgbackrest info
        EXITCODE=$?
        if [ ${EXITCODE} -ne 0 ]; then
            exit $EXITCODE
        fi
    
        pgbackrest --log-level-console=detail restore
        EXITCODE=$?
        if [ ${EXITCODE} -eq 0 ]; then
            log "pgBackRest restore finished succesfully, starting instance in recovery"
            # We want to ensure we do not overwrite a current backup repository with archives, therefore
            # we block archiving from succeeding until Patroni can takeover
            touch "${PGDATA}/recovery.signal"
            pg_ctl -D "${PGDATA}" start -o '--archive-command=/bin/false'
    
            while ! pg_isready -q; do
                log "Waiting for PostgreSQL to become available"
                sleep 3
            done
    
            # It is not trivial to figure out to what point we should restore, pgBackRest
            # should be fetching WAL segments until the WAL is exhausted. We'll ask pgBackRest
            # what the Maximum Wal is that it currently has; as soon as we see that, we can consider
            # the restore to be done
            while true; do
                MAX_BACKUP_WAL="$(pgbackrest info --output=json | python3 -c "import json,sys;obj=json.load(sys.stdin); print(obj[0]['archive'][0]['max']);")"
                log "Testing whether WAL file ${MAX_BACKUP_WAL} has been restored ..."
                [ -f "${PGDATA}/pg_wal/${MAX_BACKUP_WAL}" ] && break
                sleep 30;
            done
    
            # At this point we know the final WAL archive has been restored, we should be done.
            log "The WAL file ${MAX_BACKUP_WAL} has been successully restored, shutting down instance"
            pg_ctl -D "${PGDATA}" promote
            pg_ctl -D "${PGDATA}" stop -m fast
            log "Handing over control to Patroni ..."
        else
            log "Bootstrap from backup failed"
            exit 1
        fi
    else
        # Patroni attaches --scope and --datadir to the arguments, we need to strip them off as
        # initdb has no business with these parameters
        initdb_args=""
        for value in "$@"
        do
            case $value in
                "--scope"*)
                    ;;
                "--datadir"*)
                    ;;
                *)
                    initdb_args="${initdb_args} $value"
                    ;;
            esac
        done
    
        log "Invoking initdb"
        # shellcheck disable=SC2086
        initdb --auth-local=peer --auth-host=md5 --pgdata="${PGDATA}" --waldir="${WALDIR}" ${initdb_args}
    fi
    
    echo "include_if_exists = '${TSTUNE_FILE}'" >> "${PGDATA}/postgresql.conf"
    
  post_init.sh: |-
    #!/bin/sh
    : "${ENV_FILE:=${HOME}/.pod_environment}"
    if [ -f "${ENV_FILE}" ]; then
        echo "Sourcing ${ENV_FILE}"
        . "${ENV_FILE}"
    fi
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - post_init - $1"
    }
    
    log "Creating extension TimescaleDB in template1 and postgres databases"
    psql -d "$URL" <<__SQL__
        \connect template1
        -- As we're still only initializing, we cannot have synchronous_commit enabled just yet.
        SET synchronous_commit to 'off';
        CREATE EXTENSION timescaledb;
    
        \connect postgres
        SET synchronous_commit to 'off';
        CREATE EXTENSION timescaledb;
    __SQL__
    
    # POSTGRES_TABLESPACES is a comma-separated list of tablespaces to create
    # variable is passed in StatefulSet template
    : "${POSTGRES_TABLESPACES:=""}"
    for tablespace in $POSTGRES_TABLESPACES
    do
        log "Creating tablespace ${tablespace}"
        tablespacedir="${PGDATA}/tablespaces/${tablespace}/data"
        psql -d "$URL" --set tablespace="${tablespace}" --set directory="${tablespacedir}" --set ON_ERROR_STOP=1 <<__SQL__
        SET synchronous_commit to 'off';
        CREATE TABLESPACE :"tablespace" LOCATION :'directory';
    __SQL__
    done
    
    # This directory may contain user defined post init steps
    for file in /etc/timescaledb/post_init.d/*
    do
        [ -d "$file" ] && continue
        [ ! -r "$file" ] && continue
    
        case "$file" in
        *.sh)
            if [ -x "$file" ]; then
            log "Call post init script [ $file ]"
            "$file" "$@"
            EXITCODE=$?
            else
            log "Source post init script [ $file ]"
            . "$file"
            EXITCODE=$?
            fi
            ;;
        *.sql)
            log "Apply post init sql [ $file ]"
            # Disable synchronous_commit since we're initializing
            PGOPTIONS="-c synchronous_commit=local" psql -d "$URL" -f "$file"
            EXITCODE=$?
            ;;
        *.sql.gz)
            log "Decompress and apply post init sql [ $file ]"
            gunzip -c "$file" | PGOPTIONS="-c synchronous_commit=local" psql -d "$URL"
            EXITCODE=$?
            ;;
        *)
            log "Ignore unknown post init file type [ $file ]"
            EXITCODE=0
            ;;
        esac
        EXITCODE=$?
        if [ "$EXITCODE" != "0" ]
        then
            log "ERROR: post init script $file exited with exitcode $EXITCODE"
            exit $EXITCODE
        fi
    done
    
    # We exit 0 this script, otherwise the database initialization fails.
    exit 0
    
  patroni_callback.sh: |-
    #!/bin/sh
    set -e
    
    : "${ENV_FILE:=${HOME}/.pod_environment}"
    if [ -f "${ENV_FILE}" ]; then
        echo "Sourcing ${ENV_FILE}"
        . "${ENV_FILE}"
    fi
    
    for suffix in "$1" all
    do
        CALLBACK="/etc/timescaledb/callbacks/${suffix}"
        if [ -f "${CALLBACK}" ]
        then
        "${CALLBACK}" "$@"
        fi
    done
    
  lifecycle_preStop.sql: |-
    -- Doing a checkpoint (at the primary and the current instance) before starting
    -- the shutdown process will speed up the CHECKPOINT that is part of the shutdown
    -- process and the recovery after the pod is rescheduled.
    --
    -- We issue the CHECKPOINT at the primary always because:
    --
    -- > Restartpoints can't be performed more frequently than checkpoints in the
    -- > master because restartpoints can only be performed at checkpoint records.
    -- https://www.postgresql.org/docs/current/wal-configuration.html
    --
    -- While we're doing these preStop CHECKPOINTs we can still serve read/write
    -- queries to clients, whereas as soon as we initiate the shutdown, we terminate
    -- connections.
    --
    -- This therefore reduces downtime for the clients, at the cost of increasing (slightly)
    -- the time to stop the pod, and reducing write performance on the primary.
    --
    -- To further reduce downtime for clients, we will issue a switchover iff we are currently
    -- running as the primary. This again should be relatively fast, as we've just issued and
    -- waited for the CHECKPOINT to complete.
    --
    -- This is quite a lot of logic and work in a preStop command; however, if the preStop command
    -- fails for whatever reason, the normal Pod shutdown will commence, so it is only able to
    -- improve stuff without being able to break stuff.
    -- (The $(hostname) inside the switchover call safeguards that we never accidentally
    -- switchover the wrong primary).
    
    \pset pager off
    \set ON_ERROR_STOP true
    \set hostname `hostname`
    \set dsn_fmt 'user=postgres host=%s application_name=lifecycle:preStop@%s connect_timeout=5 options=''-c log_min_duration_statement=0'''
    
    SELECT
        pg_is_in_recovery() AS in_recovery,
        format(:'dsn_fmt', patroni_scope,                       :'hostname') AS primary_dsn,
        format(:'dsn_fmt', '/var/run/postgresql', :'hostname') AS local_dsn
    FROM
        current_setting('cluster_name') AS cs(patroni_scope)
    \gset
    
    \timing on
    \set ECHO queries
    
    -- There should be a CHECKPOINT at the primary
    \if :in_recovery
        \connect :"primary_dsn"
        CHECKPOINT;
    \endif
    
    -- There should also be a CHECKPOINT locally,
    -- for the primary, this may mean we do a double checkpoint,
    -- but the second one would be cheap anyway, so we leave that as is
    \connect :"local_dsn"
    SELECT 'Issuing checkpoint';
    CHECKPOINT;
    
    \if :in_recovery
        SELECT 'We are a replica: Successfully invoked checkpoints at the primary and locally.';
    \else
        SELECT 'We are a primary: Successfully invoked checkpoints, now issuing a switchover.';
        \! curl -s http://localhost:8008/switchover -XPOST -d '{"leader": "$(hostname)"}'
    \endif
    
...
---
# Source: timescaledb-single/templates/role-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: timescale-timescaledb
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: rbac
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  verbs:
  - create
  - get
  - list
  - patch
  - update
  - watch
  # delete is required only for 'patronictl remove'
  - delete
- apiGroups: [""]
  resources:
  - endpoints
  - endpoints/restricted
  verbs:
  - create
  - get
  - patch
  - update
  # the following three privileges are necessary only when using endpoints
  - list
  - watch
  # delete is required only for for 'patronictl remove'
  - delete
- apiGroups: [""]
  resources: ["pods"]
  verbs:
  - get
  - list
  - patch
  - update
  - watch
---
# Source: timescaledb-single/templates/rolebinding-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: timescale-timescaledb
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: rbac
subjects:
  - kind: ServiceAccount
    name: timescale-timescaledb
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: timescale-timescaledb
---
# Source: timescaledb-single/templates/svc-timescaledb-config.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: v1
kind: Service
metadata:
  name: timescale-timescaledb-config
  namespace: cityos
  labels:
    component: patroni
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: patroni
spec:
  selector:
    app: timescale-timescaledb
    cluster-name: timescale-timescaledb
  type: ClusterIP
  clusterIP: None
  ports:
  - name: patroni
    port: 8008
    protocol: TCP
---
# Source: timescaledb-single/templates/svc-timescaledb-replica.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: v1
kind: Service
metadata:
  name: timescale-timescaledb-replica
  namespace: cityos
  labels:
    component: postgres
    role: replica
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: postgres
spec:
  selector:
    app: timescale-timescaledb
    cluster-name: timescale-timescaledb
    role: replica
  type: ClusterIP
  ports:
  - name: postgresql
    # This always defaults to 5432
    port: 5432
    targetPort: postgresql
    protocol: TCP
---
# Source: timescaledb-single/templates/svc-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: v1
kind: Service
metadata:
  name: timescale-timescaledb
  namespace: cityos
  labels:
    role: master
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: timescaledb
spec:
  selector:
    app: timescale-timescaledb
    cluster-name: timescale-timescaledb
    role: master
  type: ClusterIP
  ports:
  - name: postgresql
    # This always defaults to 5432
    port: 5432
    targetPort: postgresql
    protocol: TCP
---
# Source: timescaledb-single/templates/statefulset-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: timescale-timescaledb
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: timescaledb
spec:
  serviceName: timescale-timescaledb
  replicas: 1
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: timescale-timescaledb
      release: timescale
  template:
    metadata:
      name: timescale-timescaledb
      labels:
        
        app: timescale-timescaledb
        chart: timescaledb-single-0.33.1
        release: timescale
        heritage: Helm
        cluster-name: timescale-timescaledb
        app.kubernetes.io/name: "timescale-timescaledb"
        app.kubernetes.io/version: 0.33.1
        app.kubernetes.io/component: timescaledb
    spec:
      serviceAccountName: timescale-timescaledb
      securityContext:
        # The postgres user inside the TimescaleDB image has uid=1000.
        # This configuration ensures the permissions of the mounts are suitable
        fsGroup: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
      initContainers:
      - name: tstune
        securityContext:
          allowPrivilegeEscalation: false
        image: "timescale/timescaledb-ha:pg14.6-ts2.9.1-p1"
        env:
        - name: TSTUNE_FILE
          value: /var/run/postgresql/timescaledb.conf
        - name: WAL_VOLUME_SIZE
          value: 1Gi
        - name: DATA_VOLUME_SIZE
          value: 2Gi
        - name: RESOURCES_CPU_REQUESTS
          valueFrom:
            resourceFieldRef:
              containerName: timescaledb
              resource: requests.cpu
              divisor: "1"
        - name: RESOURCES_MEMORY_REQUESTS
          valueFrom:
            resourceFieldRef:
              containerName: timescaledb
              resource: requests.memory
              divisor: 1Mi
        - name: RESOURCES_CPU_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: timescaledb
              resource: limits.cpu
              divisor: "1"
        - name: RESOURCES_MEMORY_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: timescaledb
              resource: limits.memory
              divisor: 1Mi
        # Command below will run the timescaledb-tune utility and configure min/max wal size based on PVCs size
        command:
          - sh
          - "-c"
          - '/etc/timescaledb/scripts/tstune.sh '
        volumeMounts:
        - name: socket-directory
          mountPath: /var/run/postgresql
        - name: timescaledb-scripts
          mountPath: /etc/timescaledb/scripts
          readOnly: true
        resources:
          
          {}
      # Issuing the final checkpoints on a busy database may take considerable time.
      # Unfinished checkpoints will require more time during startup, so the tradeoff
      # here is time spent in shutdown/time spent in startup.
      # We choose shutdown here, especially as during the largest part of the shutdown
      # we can still serve clients.
      terminationGracePeriodSeconds: 600
      containers:
      - name: timescaledb
        securityContext:
          allowPrivilegeEscalation: false
        image: "timescale/timescaledb-ha:pg14.6-ts2.9.1-p1"
        imagePullPolicy: Always
        lifecycle:
          preStop:
            exec:
              command:
              - psql
              - -X
              - --file
              - "/etc/timescaledb/scripts/lifecycle_preStop.sql"
        # When reusing an already existing volume it sometimes happens that the permissions
        # of the PGDATA and/or wal directory are incorrect. To guard against this, we always correctly
        # set the permissons of these directories before we hand over to Patroni.
        # We also create all the tablespaces that are defined, to ensure a smooth restore/recovery on a
        # pristine set of Volumes.
        # As PostgreSQL requires to have full control over the permissions of the tablespace directories,
        # we create a subdirectory "data" in every tablespace mountpoint. The full path of every tablespace
        # therefore always ends on "/data".
        # By creating a .pgpass file in the $HOME directory, we expose the superuser password
        # to processes that may not have it in their environment (like the preStop lifecycle hook).
        # To ensure Patroni will not mingle with this file, we give Patroni its own pgpass file.
        # As these files are in the $HOME directory, they are only available to *this* container,
        # and they are ephemeral.
        command:
          - /bin/bash
          - "-c"
          - |
            
            install -o postgres -g postgres -d -m 0700 "/var/lib/postgresql/data" "/var/lib/postgresql/wal/pg_wal" || exit 1
            TABLESPACES=""
            for tablespace in ; do
              install -o postgres -g postgres -d -m 0700 "/var/lib/postgresql/tablespaces/${tablespace}/data"
            done

            # Environment variables can be read by regular users of PostgreSQL. Especially in a Kubernetes
            # context it is likely that some secrets are part of those variables.
            # To ensure we expose as little as possible to the underlying PostgreSQL instance, we have a list
            # of allowed environment variable patterns to retain.
            #
            # We need the KUBERNETES_ environment variables for the native Kubernetes support of Patroni to work.
            #
            # NB: Patroni will remove all PATRONI_.* environment variables before starting PostgreSQL

            # We store the current environment, as initscripts, callbacks, archive_commands etc. may require
            # to have the environment available to them
            set -o posix
            export -p > "${HOME}/.pod_environment"
            export -p | grep PGBACKREST > "${HOME}/.pgbackrest_environment"

            for UNKNOWNVAR in $(env | awk -F '=' '!/^(PATRONI_.*|HOME|PGDATA|PGHOST|LC_.*|LANG|PATH|KUBERNETES_SERVICE_.*|AWS_ROLE_ARN|AWS_WEB_IDENTITY_TOKEN_FILE)=/ {print $1}')
            do
                unset "${UNKNOWNVAR}"
            done

            touch /var/run/postgresql/timescaledb.conf
            touch /var/run/postgresql/wal_status

            echo "*:*:*:postgres:${PATRONI_SUPERUSER_PASSWORD}" >> ${HOME}/.pgpass
            chmod 0600 ${HOME}/.pgpass

            export PATRONI_POSTGRESQL_PGPASS="${HOME}/.pgpass.patroni"

            exec patroni /etc/timescaledb/patroni.yaml
        env:
        # We use mixed case environment variables for Patroni User management,
        # as the variable themselves are documented to be PATRONI_<username>_OPTIONS.
        # Where possible, we want to have lowercase usernames in PostgreSQL as more complex postgres usernames
        # requiring quoting to be done in certain contexts, which many tools do not do correctly, or even at all.
        # https://patroni.readthedocs.io/en/latest/ENVIRONMENT.html#bootstrap-configuration
        - name: PATRONICTL_CONFIG_FILE
          value: "/etc/timescaledb/patroni.yaml"
        - name: PATRONI_admin_OPTIONS
          value: createrole,createdb
        - name: PATRONI_REPLICATION_USERNAME
          value: standby
        # To specify the PostgreSQL and Rest API connect addresses we need
        # the PATRONI_KUBERNETES_POD_IP to be available as a bash variable, so we can compose an
        # IP:PORT address later on
        - name: PATRONI_KUBERNETES_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PATRONI_POSTGRESQL_CONNECT_ADDRESS
          value: "$(PATRONI_KUBERNETES_POD_IP):5432"
        - name: PATRONI_RESTAPI_CONNECT_ADDRESS
          value: "$(PATRONI_KUBERNETES_POD_IP):8008"
        - name: PATRONI_KUBERNETES_PORTS
          value: '[{"name": "postgresql", "port": 5432}]'
        - name: PATRONI_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: PATRONI_POSTGRESQL_DATA_DIR
          value: "/var/lib/postgresql/data"
        - name: PATRONI_KUBERNETES_NAMESPACE
          value: cityos
        - name: PATRONI_KUBERNETES_LABELS
          value: "{app: timescale-timescaledb, cluster-name: timescale-timescaledb, release: timescale}"
        - name: PATRONI_SCOPE
          value: timescale-timescaledb
        - name: PGBACKREST_CONFIG
          value: /etc/pgbackrest/pgbackrest.conf
        # PGDATA and PGHOST are not required to let Patroni/PostgreSQL run correctly,
        # but for interactive sessions, callbacks and PostgreSQL tools they should be correct.
        - name: PGDATA
          value: "$(PATRONI_POSTGRESQL_DATA_DIR)"
        - name: PGHOST
          value: "/var/run/postgresql"
        - name: WALDIR
          value: "/var/lib/postgresql/wal/pg_wal"
        - name: BOOTSTRAP_FROM_BACKUP
          value: "0"
        - name: PGBACKREST_BACKUP_ENABLED
          value: "false"
        - name: TSTUNE_FILE
          value: /var/run/postgresql/timescaledb.conf
          # pgBackRest is also called using the archive_command if the backup is enabled.
          # this script will also need access to the environment variables specified for
          # the backup. This can be removed once we do not directly invoke pgBackRest
          # from inside the TimescaleDB container anymore
        envFrom:
        - secretRef:
            name: "timescale-timescaledb-credentials"
            optional: false
        - secretRef:
            name: "timescale-timescaledb-pgbackrest"
            optional: true
        ports:
        - containerPort: 8008
          name: patroni
        - containerPort: 5432
          name: postgresql
        readinessProbe:
          exec:
            command:
              - pg_isready
              - -h
              - /var/run/postgresql
          initialDelaySeconds: 5
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 6
        volumeMounts:
        - name: storage-volume
          mountPath: "/var/lib/postgresql"
          subPath: ""
        - name: wal-volume
          mountPath: "/var/lib/postgresql/wal"
          subPath: ""
        - mountPath: /etc/timescaledb/patroni.yaml
          subPath: patroni.yaml
          name: patroni-config
          readOnly: true
        - mountPath: /etc/timescaledb/scripts
          name: timescaledb-scripts
          readOnly: true
        - mountPath: /etc/pgbackrest_secrets
          name: pgbackrest-secrets
          readOnly: true
        - mountPath: "/etc/timescaledb/post_init.d"
          name: post-init
          readOnly: true
        - mountPath: /etc/certificate
          name: certificate
          readOnly: true
        - name: socket-directory
          mountPath: /var/run/postgresql
        
        - mountPath: /etc/pgbackrest
          name: pgbackrest
          readOnly: true
        - mountPath: /etc/pgbackrest/bootstrap
          name: pgbackrest-bootstrap
          readOnly: true
        resources:
          
          {}
      volumes:
      - name: socket-directory
        emptyDir: {}
      - name: patroni-config
        configMap:
          name: timescale-timescaledb-patroni
      - name: timescaledb-scripts
        configMap:
          name: timescale-timescaledb-scripts
          defaultMode: 488 # 0750 permissions
      
      - name: post-init
        projected:
          defaultMode: 0750
          sources:
            
            - configMap:
                name: custom-init-scripts
                optional: true
            - secret:
                name: custom-secret-scripts
                optional: true
      - name: pgbouncer
        configMap:
          name: timescale-timescaledb-pgbouncer
          defaultMode: 416 # 0640 permissions
          optional: true
      - name: pgbackrest
        configMap:
          name: timescale-timescaledb-pgbackrest
          defaultMode: 416 # 0640 permissions
          optional: true
      - name: pgbackrest-secrets
        secret:
          secretName: timescale-timescaledb-pgbackrest-secrets
          defaultMode: 416
          optional: true
      - name: certificate
        secret:
          secretName: "timescale-timescaledb-certificate"
          defaultMode: 416 # 0640 permissions
      - name: pgbackrest-bootstrap
        secret:
          secretName: pgbackrest-bootstrap
          optional: True
  volumeClaimTemplates:
    - metadata:
        name: storage-volume
        annotations:
        labels:
          app: timescale-timescaledb
          release: timescale
          heritage: Helm
          cluster-name: timescale-timescaledb
          purpose: data-directory
      spec:
        accessModes:
          
        - ReadWriteOnce
        resources:
          requests:
            storage: "2Gi"
    - metadata:
        name: wal-volume
        annotations:
        labels:
          app: timescale-timescaledb
          release: timescale
          heritage: Helm
          cluster-name: timescale-timescaledb
          purpose: wal-directory
      spec:
        accessModes:
          
        - ReadWriteOnce
        resources:
          requests:
            storage: "1Gi"
---
# Source: timescaledb-single/templates/configmap-pgbackrest.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
---
# Source: timescaledb-single/templates/configmap-pgbouncer.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
---
# Source: timescaledb-single/templates/pgbackrest.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
---
# Source: timescaledb-single/templates/secret-certificate.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: v1
kind: Secret
metadata:
  name: "timescale-timescaledb-certificate"
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: certificates
  annotations:
    "helm.sh/hook": pre-install,post-delete
    "helm.sh/hook-weight": "0"
type: kubernetes.io/tls
stringData:
  tls.crt: "-----BEGIN CERTIFICATE-----\nMIIDLDCCAhSgAwIBAgIRAPOjxyON+XCIZNe/kkJIP48wDQYJKoZIhvcNAQELBQAw\nIDEeMBwGA1UEAxMVdGltZXNjYWxlLXRpbWVzY2FsZWRiMB4XDTIzMDgxMzE4MjE0\nMVoXDTI4MDgxMjE4MjE0MVowIDEeMBwGA1UEAxMVdGltZXNjYWxlLXRpbWVzY2Fs\nZWRiMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAvEmHuCjCBBcX7qTE\nT7h1Lljhqlsj4RSppCaVB8r6DFFqT4/wyNj9Cw5Po6vdpZso1up4Pq3yD6i7HrUD\nN56MCtmGKpq1dxIB4PqNjsuPghQi9nK+21gT0u41nS8SXFcFagU80T7ZQIL6gTqt\nJGocSN3A7cEQvhIs7raAtG5yw9V+XnNKO8uisKytK8UUfnTkzKGc1+0QVflrsXUc\nH3NPXvepr8hg4Lcm9uTKo4YbYt2oX5qxBocK0m77doaf1zcJh9LNkOcBRUd4DCH4\n8WeA4oX5trDkkZiOnz+S1KmUhM2IAx+ijN9diCOrEtqQizNchy9RU3DhPkvwaVAW\nVoAEKQIDAQABo2EwXzAOBgNVHQ8BAf8EBAMCAqQwHQYDVR0lBBYwFAYIKwYBBQUH\nAwEGCCsGAQUFBwMCMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYEFBwqbGKXsnsV\nMgZ7ACAjRIyeJQD8MA0GCSqGSIb3DQEBCwUAA4IBAQCH4eJd2VtUR0EY5Z+gNI7x\nbreI2tBmQCFyXDaMcMFNjHj70j7Himwae1zgAx06X4vDwJmzTF0fTQU6ZFwOwKQk\nxNqxsaYFByw9Qwb/z1MUU5vIDV6K/QYykCR3D+VnFn8M8zGyW+Gh1zpl5SAaqUu0\njws/oja4QtcWNIUGk+sKd7O6GyHUldNj7Ne8v5TW7PIPorbAfgYE2IKE3fCmIMdZ\nGvZM73CzvhcVTBFZCH+hANaJVRJPV2PxHTrpmXxFAzYkTOxz2NYpvAsypTq15UvS\nU1FnlUs8SiNHQ61gt0dnTWqq9ChBEKbskqDZdh8Jt52XNdccP+sJNI64RGEmB2BD\n-----END CERTIFICATE-----\n"
  tls.key: "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEAvEmHuCjCBBcX7qTET7h1Lljhqlsj4RSppCaVB8r6DFFqT4/w\nyNj9Cw5Po6vdpZso1up4Pq3yD6i7HrUDN56MCtmGKpq1dxIB4PqNjsuPghQi9nK+\n21gT0u41nS8SXFcFagU80T7ZQIL6gTqtJGocSN3A7cEQvhIs7raAtG5yw9V+XnNK\nO8uisKytK8UUfnTkzKGc1+0QVflrsXUcH3NPXvepr8hg4Lcm9uTKo4YbYt2oX5qx\nBocK0m77doaf1zcJh9LNkOcBRUd4DCH48WeA4oX5trDkkZiOnz+S1KmUhM2IAx+i\njN9diCOrEtqQizNchy9RU3DhPkvwaVAWVoAEKQIDAQABAoIBAHM0+powNiaSis42\nevfcgbbOI6Y9BgMexX3vLZkLFVycJFRyWn5KZupw40HeU0yVV7PI27JedJIoR/ZO\nA53woPeyiWlgKFCm8Adcx2jsHO8Wqx3or+c+SULzBlMbDjz58hIhCkluDzht7CFO\nacqyMoRTS00uqLPkIyIdATtlqUsxNXAd/tvb3VS77oA0Y7mvQqzc9qkSVdWCrJYE\nsNosdo8TbF1rxkwViT/abf1UHBjv6cXi8qp39/RX9bJgoAjNOJ758CS8Vl7a6n62\nlJza1okSIB6AHo58aQQYlrwwEyABifUAgTPOAYgVDWdBXT5jLhkNNVPP3gHqk9Wz\n1v/0r9ECgYEAzusB9q3vZ7U9bPnC2v+a8q4+w3/3bLazYCQAMp/HhRwp3mdMvkE2\nOPIvyn/xeVV5ILwNl5NbUwJzLA5Hf76bLRVIVxSWSGf8j6bXIApzPqDU6jMMkUyM\n80K4NJLH9L3aDH46y/O2l/39Kk5TODA+Fy8HPmL6GvLgZAlv1MgOje8CgYEA6PMt\nnGwpI0FYMk2EqesA4tjnkD8BHKOWAtCVRgyLr7tY8bWst7D4r5LX5Y6w/p084WMR\nWQAWiHdmNNJpPqDWExFpF7q4/fHkXxpWPZ7RZ+XB2xWhbtUGWYAdrnYiy/rPkO1/\n9NTEBeCf+BIXAiWVtiLXamTmtqe0PRU8YtTSp2cCgYA7b4Pj4Lh4+LWJPQD8uRlA\nBk8fBv4Ca0SBJDYuVHkSQjHAobmImODDYMXbi/7H1IZzobDhzkEYSgbdK8jb59tZ\nCtj5sn8skzoX5Vu752SNI9Ok5j8A6ZU/NvYEm/j10yb0Gbo35WyYoit7YH5FEI51\nSFvzTqj63rMX6SIHY+PSXwKBgQC0WclfSHIVBHs2/sMOFo5qEoo9KDpBJPl4Fmrr\nf70KAd8by4777Hys2eBirrWjerLuwJdms2W+o8oybRDkVvtGaf1WTM76dLNdWfOY\n44O2j7AlYTrJpw2ybzkLoBTSnczjpKyjLbLR5hWTptW/aL/e8lya16GzmmU1wWIu\nFh09UwKBgHI+dz+RQykxO+bOheSHOUCp+F9Np+Kt7/oaYzi3TCB1hoaz80x8KiEI\nZFeTlhGoiJERqu9z0wUFoTrZdfKIWGvqc/YJyLnA59NTxMqadItyKcA01oQQEs4m\nUIjQ8AWQvy8JLQVJRZPuJtwKIA4yHfuAxbWfg89i/CX986Qm8XUF\n-----END RSA PRIVATE KEY-----\n"
...
---
# Source: timescaledb-single/templates/secret-patroni.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: v1
kind: Secret
metadata:
  name: "timescale-timescaledb-credentials"
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: patroni
  annotations:
    "helm.sh/hook": pre-install,post-delete
    "helm.sh/hook-weight": "0"
    "helm.sh/resource-policy": keep
type: Opaque
stringData:
  PATRONI_SUPERUSER_PASSWORD: "XOYRYjoOBe9aYiuV"
  PATRONI_REPLICATION_PASSWORD: "o0wKJGY3Foh5zYeZ"
  PATRONI_admin_PASSWORD: "y6mtW8UfnRWrC40y"
...
---
# Source: timescaledb-single/templates/secret-pgbackrest.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: v1
kind: Secret
metadata:
  name: "timescale-timescaledb-pgbackrest"
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: pgbackrest
  annotations:
    "helm.sh/hook": pre-install,post-delete
    "helm.sh/hook-weight": "0"
    "helm.sh/resource-policy": keep
type: Opaque
stringData:
  {}
...
---
# Source: timescaledb-single/templates/job-update-patroni.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: batch/v1
kind: Job
metadata:
  name: "timescale-timescaledb-patroni-1"
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: patroni
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  activeDeadlineSeconds: 120
  template:
    metadata:
      labels:
        
        app: timescale-timescaledb
        chart: timescaledb-single-0.33.1
        release: timescale
        heritage: Helm
        cluster-name: timescale-timescaledb
        app.kubernetes.io/name: "timescale-timescaledb"
        app.kubernetes.io/version: 0.33.1
    spec:
      restartPolicy: OnFailure
      containers:
      - name: timescale-timescaledb-patch-patroni-config
        image: "curlimages/curl:7.87.0"
        imagePullPolicy: Always
        command: ["/bin/sh"]
        # Patching the Patroni configuration is good, however it should not block an upgrade from going through
        # Therefore we ensure we always exit with an exitcode 0, so that Helm is satisfied with this upgrade job
        args:
        - '-c'
        - |
          /usr/bin/curl --connect-timeout 30 --include --request PATCH --data \
          "{\"loop_wait\":10,\"maximum_lag_on_failover\":33554432,\"postgresql\":{\"parameters\":{\"archive_command\":\"/etc/timescaledb/scripts/pgbackrest_archive.sh %p\",\"archive_mode\":\"on\",\"archive_timeout\":\"1800s\",\"autovacuum_analyze_scale_factor\":0.02,\"autovacuum_max_workers\":10,\"autovacuum_naptime\":\"5s\",\"autovacuum_vacuum_cost_limit\":500,\"autovacuum_vacuum_scale_factor\":0.05,\"hot_standby\":\"on\",\"log_autovacuum_min_duration\":\"1min\",\"log_checkpoints\":\"on\",\"log_connections\":\"on\",\"log_disconnections\":\"on\",\"log_line_prefix\":\"%t [%p]: [%c-%l] %u@%d,app=%a [%e] \",\"log_lock_waits\":\"on\",\"log_min_duration_statement\":\"1s\",\"log_statement\":\"ddl\",\"max_connections\":100,\"max_prepared_transactions\":150,\"shared_preload_libraries\":\"timescaledb,pg_stat_statements\",\"ssl\":\"on\",\"ssl_cert_file\":\"/etc/certificate/tls.crt\",\"ssl_key_file\":\"/etc/certificate/tls.key\",\"tcp_keepalives_idle\":900,\"tcp_keepalives_interval\":100,\"temp_file_limit\":\"1GB\",\"timescaledb.passfile\":\"../.pgpass\",\"unix_socket_directories\":\"/var/run/postgresql\",\"unix_socket_permissions\":\"0750\",\"wal_level\":\"hot_standby\",\"wal_log_hints\":\"on\"},\"use_pg_rewind\":true,\"use_slots\":true},\"retry_timeout\":10,\"ttl\":30}" \
          "http://timescale-timescaledb-config:8008/config"
          exit 0



---
# Source: pgadmin4/templates/networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
spec:
  policyTypes:
    - Ingress
  podSelector:
    matchLabels:
      app.kubernetes.io/name: pgadmin4
      app.kubernetes.io/instance: pgadmin4
  ingress:
  - ports:
    - port: 80
---
# Source: pgadmin4/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  password: "U3VwZXJTZWNyZXQ="
---
# Source: pgadmin4/templates/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
# Source: pgadmin4/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: pgadmin4
    app.kubernetes.io/instance: pgadmin4
---
# Source: pgadmin4/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: pgadmin4
      app.kubernetes.io/instance: pgadmin4
  template:
    metadata:
      labels:
        app.kubernetes.io/name: pgadmin4
        app.kubernetes.io/instance: pgadmin4
      annotations:
        checksum/secret: 2acaa91f3a0987d847c541fcd7622f6419a80506088a14f3e4821dbb16fd3a9f

    spec:
      containers:
        - name: pgadmin4
          image: "docker.io/dpage/pgadmin4:7.4"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              port: http
              path: "/pgadmin4/misc/ping"
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 15
          readinessProbe:
            httpGet:
              port: http
              path: "/pgadmin4/misc/ping"
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 60
            successThreshold: 1
            timeoutSeconds: 15
          env:
            - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION
              value: "False"
            - name: PGADMIN_DEFAULT_EMAIL
              value: chart@domain.com
            - name: PGADMIN_DEFAULT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: pgadmin4
                  key: password
            - name: SCRIPT_NAME
              value: /pgadmin4
          volumeMounts:
            - name: pgadmin-data
              mountPath: /var/lib/pgadmin
          resources:
            {}
      volumes:
        - name: pgadmin-data
          persistentVolumeClaim:
            claimName: pgadmin4
      securityContext:
        fsGroup: 5050
        runAsGroup: 5050
        runAsUser: 5050
---
# Source: pgadmin4/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: pgadmin4
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
spec:
  rules:
    - host: "anett-xmg-fusion-15-xfu15l19"
      http:
        paths:
          - path: /pgadmin4
            pathType: Prefix
            backend:
              service:
                name: pgadmin4
                port:
                  number: 80
---
# Source: pgadmin4/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "pgadmin4-test-connection"
  namespace: cityos
  labels:
    app.kubernetes.io/name: pgadmin4
    helm.sh/chart: pgadmin4-1.15.4
    app.kubernetes.io/version: "7.4"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  securityContext:
    runAsNonRoot: true
    fsGroup: 5051
    runAsGroup: 5051
    runAsUser: 5051
  containers:
    - name: wget
      image: "docker.io/busybox:latest"
      env:
        - name: PGADMIN_HOST
          value: pgadmin4
        - name: PGADMIN_PORT
          value: "80"
      command:
        - /bin/sh
        - -ec
        - |
          response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)
          check=$(echo $response | grep -c '200 OK'); echo $check; if [[ $check -gt 0 ]]; then echo "Response OK"; else exit 1; fi
      resources:
        {}
      securityContext:
        readOnlyRootFilesystem: true
  restartPolicy: Never

---
# Source: timescale-config/templates/custom-secret-scripts.yaml
apiVersion: v1
kind: Secret
metadata:
  name: custom-secret-scripts
type: Opaque
data:
  timescale-secrets.sh: IyEvYmluL2Jhc2gKcHNxbCAtZCAiJDEiIC0tZmlsZT0tIC0tc2V0IE9OX0VSUk9SX1NUT1A9MSA8PCBfX1NRTF9fClNFVCBsb2dfc3RhdGVtZW50IFRPIG5vbmU7ICAgICAgLS0gcHJldmVudCB0aGVzZSBwYXNzd29yZHMgZnJvbSBiZWluZyBsb2dnZWQKQUxURVIgVVNFUiBoYWNrYXRob24gIFdJVEggUEFTU1dPUkQgJ2hhY2thdGhvbic7CkFMVEVSIFVTRVIgY2l0eWRhc2hib2FyZCAgV0lUSCBQQVNTV09SRCAnY2l0eWRhc2hib2FyZCc7Cl9fU1FMX18K
---
# Source: timescale-config/templates/custom-init-scripts.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-init-scripts
data:
  custom-config.sh: |-
    #!/bin/bash

    psql -d "$1" <<__SQL__
    CREATE ROLE hackathon WITH LOGIN;
    CREATE DATABASE hackathon OWNER hackathon;
    __SQL__

    psql -d "$1" <<__SQL__
    CREATE ROLE citydashboard WITH LOGIN;
    CREATE DATABASE citydashboard OWNER citydashboard;
    __SQL__

---
# Source: busybox/templates/pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - image: radial/busyboxplus:curl
    command:
      - sleep
      - "3600"
    imagePullPolicy: IfNotPresent
    name: busybox
  #   volumeMounts:
  #   - mountPath: /
  #     name: logging-peristent-volume
  # volumes:
  #   - name: logging-peristent-volume
  #     persistentVolumeClaim:
  #       claimName: logging-volume-claim
  restartPolicy: Always

---
# Source: secrets/templates/ghcr-starwit-secret.yaml
apiVersion: v1
data:
  .dockerconfigjson: eyJhdXRocyI6eyJnaGNyLmlvIjp7InVzZXJuYW1lIjoid2l0Y2hwb3UiLCJwYXNzd29yZCI6ImdocF9CY3hPTXhYakgwSEhxRjdWeURNREhjSWtqdUNqYkgxNHNVdloiLCJhdXRoIjoiIn19fQ==
kind: Secret
metadata:
  creationTimestamp: null
  name: ghcr-starwit
type: kubernetes.io/dockerconfigjson

---
# Source: timescaledb-single/templates/serviceaccount-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: v1
kind: ServiceAccount
metadata:
  name: timescale-timescaledb
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: rbac
---
# Source: timescaledb-single/templates/configmap-patroni.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.---
apiVersion: v1
kind: ConfigMap
metadata:
  name: timescale-timescaledb-patroni
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: patroni
data:
  patroni.yaml: |
    bootstrap:
      dcs:
        loop_wait: 10
        maximum_lag_on_failover: 33554432
        postgresql:
          parameters:
            archive_command: /etc/timescaledb/scripts/pgbackrest_archive.sh %p
            archive_mode: "on"
            archive_timeout: 1800s
            autovacuum_analyze_scale_factor: 0.02
            autovacuum_max_workers: 10
            autovacuum_naptime: 5s
            autovacuum_vacuum_cost_limit: 500
            autovacuum_vacuum_scale_factor: 0.05
            hot_standby: "on"
            log_autovacuum_min_duration: 1min
            log_checkpoints: "on"
            log_connections: "on"
            log_disconnections: "on"
            log_line_prefix: '%t [%p]: [%c-%l] %u@%d,app=%a [%e] '
            log_lock_waits: "on"
            log_min_duration_statement: 1s
            log_statement: ddl
            max_connections: 100
            max_prepared_transactions: 150
            shared_preload_libraries: timescaledb,pg_stat_statements
            ssl: "on"
            ssl_cert_file: /etc/certificate/tls.crt
            ssl_key_file: /etc/certificate/tls.key
            tcp_keepalives_idle: 900
            tcp_keepalives_interval: 100
            temp_file_limit: 1GB
            timescaledb.passfile: ../.pgpass
            unix_socket_directories: /var/run/postgresql
            unix_socket_permissions: "0750"
            wal_level: hot_standby
            wal_log_hints: "on"
          use_pg_rewind: true
          use_slots: true
        retry_timeout: 10
        ttl: 30
      method: restore_or_initdb
      post_init: /etc/timescaledb/scripts/post_init.sh
      restore_or_initdb:
        command: |
          /etc/timescaledb/scripts/restore_or_initdb.sh --encoding=UTF8 --locale=C.UTF-8
        keep_existing_recovery_conf: true
    kubernetes:
      role_label: role
      scope_label: cluster-name
      use_endpoints: true
    log:
      level: WARNING
    postgresql:
      authentication:
        replication:
          username: standby
        superuser:
          username: postgres
      basebackup:
      - waldir: /var/lib/postgresql/wal/pg_wal
      callbacks:
        on_reload: /etc/timescaledb/scripts/patroni_callback.sh
        on_restart: /etc/timescaledb/scripts/patroni_callback.sh
        on_role_change: /etc/timescaledb/scripts/patroni_callback.sh
        on_start: /etc/timescaledb/scripts/patroni_callback.sh
        on_stop: /etc/timescaledb/scripts/patroni_callback.sh
      create_replica_methods:
      - pgbackrest
      - basebackup
      listen: 0.0.0.0:5432
      pg_hba:
      - local     all             postgres                              peer
      - local     all             all                                   password
      - hostnossl all,replication all                all                password
      - hostssl   all             all                127.0.0.1/32       password
      - hostssl   all             all                ::1/128            password
      - hostssl   replication     standby            all                password
      - hostssl   all             all                all                password
      pgbackrest:
        command: /etc/timescaledb/scripts/pgbackrest_restore.sh
        keep_data: true
        no_master: true
        no_params: true
      recovery_conf:
        restore_command: /etc/timescaledb/scripts/pgbackrest_archive_get.sh %f "%p"
      use_unix_socket: true
    restapi:
      listen: 0.0.0.0:8008
...
---
# Source: timescaledb-single/templates/configmap-pgbackrest.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: timescale-timescaledb-pgbackrest
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: pgbackrest
data:
  pgbackrest.conf: |
    [global]
    compress-level=3
    compress-type=lz4
    process-max=4
    repo1-cipher-type=none
    repo1-path=/cityos/timescale-timescaledb/
    repo1-retention-diff=2
    repo1-retention-full=2
    spool-path=/var/run/postgresql
    start-fast=y

    [poddb]
    pg1-port=5432
    pg1-host-user=postgres
    pg1-path=/var/lib/postgresql/data
    pg1-socket-path=/var/run/postgresql

    link-all=y

    [global:archive-push]

    [global:archive-get]
...
---
# Source: timescaledb-single/templates/configmap-scripts.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.---
apiVersion: v1
kind: ConfigMap
metadata:
  name: timescale-timescaledb-scripts
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: scripts
data:
  tstune.sh: |-
    #!/bin/sh
    
    set -eu
    
    # Exit if required variable is not set externally
    : "$TSTUNE_FILE"
    : "$WAL_VOLUME_SIZE"
    : "$DATA_VOLUME_SIZE"
    : "$RESOURCES_CPU_REQUESTS"
    : "$RESOURCES_MEMORY_REQUESTS"
    : "$RESOURCES_CPU_LIMIT"
    : "$RESOURCES_MEMORY_LIMIT"
    
    # Figure out how many cores are available
    CPUS="$RESOURCES_CPU_REQUESTS"
    if [ "$RESOURCES_CPU_REQUESTS" -eq 0 ]; then
        CPUS="${RESOURCES_CPU_LIMIT}"
    fi
    # Figure out how much memory is available
    MEMORY="$RESOURCES_MEMORY_REQUESTS"
    if [ "$RESOURCES_MEMORY_REQUESTS" -eq 0 ]; then
        MEMORY="${RESOURCES_MEMORY_LIMIT}"
    fi
    
    # Ensure tstune config file exists
    touch "${TSTUNE_FILE}"
    
    # Ensure tstune-generated config is included in postgresql.conf
    if [ -f "${PGDATA}/postgresql.base.conf" ] && ! grep "include_if_exists = '${TSTUNE_FILE}'" postgresql.base.conf -qxF; then
        echo "include_if_exists = '${TSTUNE_FILE}'" >> "${PGDATA}/postgresql.base.conf"
    fi
    
    # If there is a dedicated WAL Volume, we want to set max_wal_size to 60% of that volume
    # If there isn't a dedicated WAL Volume, we set it to 20% of the data volume
    if [ "${WAL_VOLUME_SIZE}" = "0" ]; then
        WALMAX="${DATA_VOLUME_SIZE}"
        WALPERCENT=20
    else
        WALMAX="${WAL_VOLUME_SIZE}"
        WALPERCENT=60
    fi
    
    WALMAX=$(numfmt --from=auto "${WALMAX}")
    
    # Wal segments are 16MB in size, in this way we get a "nice" number of the nearest
    # 16MB
    # walmax / 100 * walpercent / 16MB # below is a refactored with increased precision
    WALMAX=$(( WALMAX * WALPERCENT * 16 / 16777216 / 100  ))
    WALMIN=$(( WALMAX / 2 ))
    
    echo "max_wal_size=${WALMAX}MB" >> "${TSTUNE_FILE}"
    echo "min_wal_size=${WALMIN}MB" >> "${TSTUNE_FILE}"
    
    # Run tstune
    timescaledb-tune -quiet -conf-path "${TSTUNE_FILE}" -cpus "${CPUS}" -memory "${MEMORY}MB" -yes "$@"
    
  pgbackrest_archive.sh: |-
    #!/bin/sh
    
    # If no backup is configured, archive_command would normally fail. A failing archive_command on a cluster
    # is going to cause WAL to be kept around forever, meaning we'll fill up Volumes we have quite quickly.
    #
    # Therefore, if the backup is disabled, we always return exitcode 0 when archiving
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - archive - $1"
    }
    
    [ -z "$1" ] && log "Usage: $0 <WALFILE or DIRECTORY>" && exit 1
    
    : "${ENV_FILE:=${HOME}/.pgbackrest_environment}"
    if [ -f "${ENV_FILE}" ]; then
        echo "Sourcing ${ENV_FILE}"
        . "${ENV_FILE}"
    fi
    
    # PGBACKREST_BACKUP_ENABLED variable is passed in StatefulSet template
    [ "${PGBACKREST_BACKUP_ENABLED}" = "true" ] || exit 0
    
    exec pgbackrest --stanza=poddb archive-push "$@"
    
  pgbackrest_archive_get.sh: |-
    #!/bin/sh
    # PGBACKREST_BACKUP_ENABLED variable is passed in StatefulSet template
    [ "${PGBACKREST_BACKUP_ENABLED}" = "true" ] || exit 1
    
    : "${ENV_FILE:=${HOME}/.pgbackrest_environment}"
    if [ -f "${ENV_FILE}" ]; then
    echo "Sourcing ${ENV_FILE}"
    . "${ENV_FILE}"
    fi
    
    exec pgbackrest --stanza=poddb archive-get "${1}" "${2}"
    
  pgbackrest_bootstrap.sh: |-
    #!/bin/sh
    set -e
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - bootstrap - $1"
    }
    
    terminate() {
        log "Stopping"
        exit 1
    }
    # If we don't catch these signals, and we're still waiting for PostgreSQL
    # to be ready, we will not respond at all to a regular shutdown request,
    # therefore, we explicitly terminate if we receive these signals.
    trap terminate TERM QUIT
    
    while ! pg_isready -q; do
        log "Waiting for PostgreSQL to become available"
        sleep 3
    done
    
    # We'll be lazy; we wait for another while to allow the database to promote
    # to primary if it's the only one running
    sleep 10
    
    # If we are the primary, we want to create/validate the backup stanza
    if [ "$(psql -c "SELECT pg_is_in_recovery()::text" -AtXq)" = "false" ]; then
        pgbackrest check || {
            log "Creating pgBackrest stanza"
            pgbackrest --stanza=poddb stanza-create --log-level-stderr=info || exit 1
            log "Creating initial backup"
            pgbackrest --type=full backup || exit 1
        }
    fi
    
    log "Starting pgBackrest api to listen for backup requests"
    exec python3 /scripts/pgbackrest-rest.py --stanza=poddb --loglevel=debug
    
  pgbackrest_restore.sh: |
    #!/bin/sh
    # PGBACKREST_BACKUP_ENABLED variable is passed in StatefulSet template
    [ "${PGBACKREST_BACKUP_ENABLED}" = "true" ] || exit 1
    
    : "${ENV_FILE:=${HOME}/.pod_environment}"
    if [ -f "${ENV_FILE}" ]; then
    echo "Sourcing ${ENV_FILE}"
    . "${ENV_FILE}"
    fi
    
    # PGDATA and WALDIR are set in the StatefulSet template and are sourced from the ENV_FILE
    # PGDATA=
    # WALDIR=
    
    # A missing PGDATA points to Patroni removing a botched PGDATA, or manual
    # intervention. In this scenario, we need to recreate the DATA and WALDIRs
    # to keep pgBackRest happy
    [ -d "${PGDATA}" ] || install -o postgres -g postgres -d -m 0700 "${PGDATA}"
    [ -d "${WALDIR}" ] || install -o postgres -g postgres -d -m 0700 "${WALDIR}"
    
    exec pgbackrest --force --delta --log-level-console=detail restore
    
  restore_or_initdb.sh: |
    #!/bin/sh
    
    : "${ENV_FILE:=${HOME}/.pod_environment}"
    if [ -f "${ENV_FILE}" ]; then
        echo "Sourcing ${ENV_FILE}"
        . "${ENV_FILE}"
    fi
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - restore_or_initdb - $1"
    }
    
    # PGDATA and WALDIR are set in the StatefulSet template and are sourced from the ENV_FILE
    # PGDATA=
    # WALDIR=
    
    # A missing PGDATA points to Patroni removing a botched PGDATA, or manual
    # intervention. In this scenario, we need to recreate the DATA and WALDIRs
    # to keep pgBackRest happy
    [ -d "${PGDATA}" ] || install -o postgres -g postgres -d -m 0700 "${PGDATA}"
    [ -d "${WALDIR}" ] || install -o postgres -g postgres -d -m 0700 "${WALDIR}"
    
    if [ "${BOOTSTRAP_FROM_BACKUP}" = "1" ]; then
        log "Attempting restore from backup"
        # we want to override the environment with the environment
        # shellcheck disable=SC2046
        export $(env -i envdir /etc/pgbackrest/bootstrap env) > /dev/null
        # PGBACKREST_REPO1_PATH is set in the StatefulSet template and sourced from the ENV_FILE
    
        if [ -z "${PGBACKREST_REPO1_PATH}" ]; then
            log "Unconfigured repository path"
            cat << "__EOT__"
    
    TimescaleDB Single Helm Chart error:
    
    You should configure the bootstrapFromBackup in your Helm Chart section by explicitly setting
    the repo1-path to point to the backups.
    
    For more information, consult the admin guide:
    https://github.com/timescale/helm-charts/blob/main/charts/timescaledb-single/docs/admin-guide.md#bootstrap-from-backup
    
    __EOT__
    
            exit 1
        fi
    
        log "Listing available backup information"
        pgbackrest info
        EXITCODE=$?
        if [ ${EXITCODE} -ne 0 ]; then
            exit $EXITCODE
        fi
    
        pgbackrest --log-level-console=detail restore
        EXITCODE=$?
        if [ ${EXITCODE} -eq 0 ]; then
            log "pgBackRest restore finished succesfully, starting instance in recovery"
            # We want to ensure we do not overwrite a current backup repository with archives, therefore
            # we block archiving from succeeding until Patroni can takeover
            touch "${PGDATA}/recovery.signal"
            pg_ctl -D "${PGDATA}" start -o '--archive-command=/bin/false'
    
            while ! pg_isready -q; do
                log "Waiting for PostgreSQL to become available"
                sleep 3
            done
    
            # It is not trivial to figure out to what point we should restore, pgBackRest
            # should be fetching WAL segments until the WAL is exhausted. We'll ask pgBackRest
            # what the Maximum Wal is that it currently has; as soon as we see that, we can consider
            # the restore to be done
            while true; do
                MAX_BACKUP_WAL="$(pgbackrest info --output=json | python3 -c "import json,sys;obj=json.load(sys.stdin); print(obj[0]['archive'][0]['max']);")"
                log "Testing whether WAL file ${MAX_BACKUP_WAL} has been restored ..."
                [ -f "${PGDATA}/pg_wal/${MAX_BACKUP_WAL}" ] && break
                sleep 30;
            done
    
            # At this point we know the final WAL archive has been restored, we should be done.
            log "The WAL file ${MAX_BACKUP_WAL} has been successully restored, shutting down instance"
            pg_ctl -D "${PGDATA}" promote
            pg_ctl -D "${PGDATA}" stop -m fast
            log "Handing over control to Patroni ..."
        else
            log "Bootstrap from backup failed"
            exit 1
        fi
    else
        # Patroni attaches --scope and --datadir to the arguments, we need to strip them off as
        # initdb has no business with these parameters
        initdb_args=""
        for value in "$@"
        do
            case $value in
                "--scope"*)
                    ;;
                "--datadir"*)
                    ;;
                *)
                    initdb_args="${initdb_args} $value"
                    ;;
            esac
        done
    
        log "Invoking initdb"
        # shellcheck disable=SC2086
        initdb --auth-local=peer --auth-host=md5 --pgdata="${PGDATA}" --waldir="${WALDIR}" ${initdb_args}
    fi
    
    echo "include_if_exists = '${TSTUNE_FILE}'" >> "${PGDATA}/postgresql.conf"
    
  post_init.sh: |-
    #!/bin/sh
    : "${ENV_FILE:=${HOME}/.pod_environment}"
    if [ -f "${ENV_FILE}" ]; then
        echo "Sourcing ${ENV_FILE}"
        . "${ENV_FILE}"
    fi
    
    log() {
        echo "$(date '+%Y-%m-%d %H:%M:%S') - post_init - $1"
    }
    
    log "Creating extension TimescaleDB in template1 and postgres databases"
    psql -d "$URL" <<__SQL__
        \connect template1
        -- As we're still only initializing, we cannot have synchronous_commit enabled just yet.
        SET synchronous_commit to 'off';
        CREATE EXTENSION timescaledb;
    
        \connect postgres
        SET synchronous_commit to 'off';
        CREATE EXTENSION timescaledb;
    __SQL__
    
    # POSTGRES_TABLESPACES is a comma-separated list of tablespaces to create
    # variable is passed in StatefulSet template
    : "${POSTGRES_TABLESPACES:=""}"
    for tablespace in $POSTGRES_TABLESPACES
    do
        log "Creating tablespace ${tablespace}"
        tablespacedir="${PGDATA}/tablespaces/${tablespace}/data"
        psql -d "$URL" --set tablespace="${tablespace}" --set directory="${tablespacedir}" --set ON_ERROR_STOP=1 <<__SQL__
        SET synchronous_commit to 'off';
        CREATE TABLESPACE :"tablespace" LOCATION :'directory';
    __SQL__
    done
    
    # This directory may contain user defined post init steps
    for file in /etc/timescaledb/post_init.d/*
    do
        [ -d "$file" ] && continue
        [ ! -r "$file" ] && continue
    
        case "$file" in
        *.sh)
            if [ -x "$file" ]; then
            log "Call post init script [ $file ]"
            "$file" "$@"
            EXITCODE=$?
            else
            log "Source post init script [ $file ]"
            . "$file"
            EXITCODE=$?
            fi
            ;;
        *.sql)
            log "Apply post init sql [ $file ]"
            # Disable synchronous_commit since we're initializing
            PGOPTIONS="-c synchronous_commit=local" psql -d "$URL" -f "$file"
            EXITCODE=$?
            ;;
        *.sql.gz)
            log "Decompress and apply post init sql [ $file ]"
            gunzip -c "$file" | PGOPTIONS="-c synchronous_commit=local" psql -d "$URL"
            EXITCODE=$?
            ;;
        *)
            log "Ignore unknown post init file type [ $file ]"
            EXITCODE=0
            ;;
        esac
        EXITCODE=$?
        if [ "$EXITCODE" != "0" ]
        then
            log "ERROR: post init script $file exited with exitcode $EXITCODE"
            exit $EXITCODE
        fi
    done
    
    # We exit 0 this script, otherwise the database initialization fails.
    exit 0
    
  patroni_callback.sh: |-
    #!/bin/sh
    set -e
    
    : "${ENV_FILE:=${HOME}/.pod_environment}"
    if [ -f "${ENV_FILE}" ]; then
        echo "Sourcing ${ENV_FILE}"
        . "${ENV_FILE}"
    fi
    
    for suffix in "$1" all
    do
        CALLBACK="/etc/timescaledb/callbacks/${suffix}"
        if [ -f "${CALLBACK}" ]
        then
        "${CALLBACK}" "$@"
        fi
    done
    
  lifecycle_preStop.sql: |-
    -- Doing a checkpoint (at the primary and the current instance) before starting
    -- the shutdown process will speed up the CHECKPOINT that is part of the shutdown
    -- process and the recovery after the pod is rescheduled.
    --
    -- We issue the CHECKPOINT at the primary always because:
    --
    -- > Restartpoints can't be performed more frequently than checkpoints in the
    -- > master because restartpoints can only be performed at checkpoint records.
    -- https://www.postgresql.org/docs/current/wal-configuration.html
    --
    -- While we're doing these preStop CHECKPOINTs we can still serve read/write
    -- queries to clients, whereas as soon as we initiate the shutdown, we terminate
    -- connections.
    --
    -- This therefore reduces downtime for the clients, at the cost of increasing (slightly)
    -- the time to stop the pod, and reducing write performance on the primary.
    --
    -- To further reduce downtime for clients, we will issue a switchover iff we are currently
    -- running as the primary. This again should be relatively fast, as we've just issued and
    -- waited for the CHECKPOINT to complete.
    --
    -- This is quite a lot of logic and work in a preStop command; however, if the preStop command
    -- fails for whatever reason, the normal Pod shutdown will commence, so it is only able to
    -- improve stuff without being able to break stuff.
    -- (The $(hostname) inside the switchover call safeguards that we never accidentally
    -- switchover the wrong primary).
    
    \pset pager off
    \set ON_ERROR_STOP true
    \set hostname `hostname`
    \set dsn_fmt 'user=postgres host=%s application_name=lifecycle:preStop@%s connect_timeout=5 options=''-c log_min_duration_statement=0'''
    
    SELECT
        pg_is_in_recovery() AS in_recovery,
        format(:'dsn_fmt', patroni_scope,                       :'hostname') AS primary_dsn,
        format(:'dsn_fmt', '/var/run/postgresql', :'hostname') AS local_dsn
    FROM
        current_setting('cluster_name') AS cs(patroni_scope)
    \gset
    
    \timing on
    \set ECHO queries
    
    -- There should be a CHECKPOINT at the primary
    \if :in_recovery
        \connect :"primary_dsn"
        CHECKPOINT;
    \endif
    
    -- There should also be a CHECKPOINT locally,
    -- for the primary, this may mean we do a double checkpoint,
    -- but the second one would be cheap anyway, so we leave that as is
    \connect :"local_dsn"
    SELECT 'Issuing checkpoint';
    CHECKPOINT;
    
    \if :in_recovery
        SELECT 'We are a replica: Successfully invoked checkpoints at the primary and locally.';
    \else
        SELECT 'We are a primary: Successfully invoked checkpoints, now issuing a switchover.';
        \! curl -s http://localhost:8008/switchover -XPOST -d '{"leader": "$(hostname)"}'
    \endif
    
...
---
# Source: timescaledb-single/templates/role-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: timescale-timescaledb
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: rbac
rules:
- apiGroups: [""]
  resources: ["configmaps"]
  verbs:
  - create
  - get
  - list
  - patch
  - update
  - watch
  # delete is required only for 'patronictl remove'
  - delete
- apiGroups: [""]
  resources:
  - endpoints
  - endpoints/restricted
  verbs:
  - create
  - get
  - patch
  - update
  # the following three privileges are necessary only when using endpoints
  - list
  - watch
  # delete is required only for for 'patronictl remove'
  - delete
- apiGroups: [""]
  resources: ["pods"]
  verbs:
  - get
  - list
  - patch
  - update
  - watch
---
# Source: timescaledb-single/templates/rolebinding-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: timescale-timescaledb
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: rbac
subjects:
  - kind: ServiceAccount
    name: timescale-timescaledb
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: timescale-timescaledb
---
# Source: timescaledb-single/templates/svc-timescaledb-config.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: v1
kind: Service
metadata:
  name: timescale-timescaledb-config
  namespace: cityos
  labels:
    component: patroni
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: patroni
spec:
  selector:
    app: timescale-timescaledb
    cluster-name: timescale-timescaledb
  type: ClusterIP
  clusterIP: None
  ports:
  - name: patroni
    port: 8008
    protocol: TCP
---
# Source: timescaledb-single/templates/svc-timescaledb-replica.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: v1
kind: Service
metadata:
  name: timescale-timescaledb-replica
  namespace: cityos
  labels:
    component: postgres
    role: replica
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: postgres
spec:
  selector:
    app: timescale-timescaledb
    cluster-name: timescale-timescaledb
    role: replica
  type: ClusterIP
  ports:
  - name: postgresql
    # This always defaults to 5432
    port: 5432
    targetPort: postgresql
    protocol: TCP
---
# Source: timescaledb-single/templates/svc-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: v1
kind: Service
metadata:
  name: timescale-timescaledb
  namespace: cityos
  labels:
    role: master
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: timescaledb
spec:
  selector:
    app: timescale-timescaledb
    cluster-name: timescale-timescaledb
    role: master
  type: ClusterIP
  ports:
  - name: postgresql
    # This always defaults to 5432
    port: 5432
    targetPort: postgresql
    protocol: TCP
---
# Source: timescaledb-single/templates/statefulset-timescaledb.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: timescale-timescaledb
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: timescaledb
spec:
  serviceName: timescale-timescaledb
  replicas: 1
  podManagementPolicy: OrderedReady
  updateStrategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app: timescale-timescaledb
      release: timescale
  template:
    metadata:
      name: timescale-timescaledb
      labels:
        
        app: timescale-timescaledb
        chart: timescaledb-single-0.33.1
        release: timescale
        heritage: Helm
        cluster-name: timescale-timescaledb
        app.kubernetes.io/name: "timescale-timescaledb"
        app.kubernetes.io/version: 0.33.1
        app.kubernetes.io/component: timescaledb
    spec:
      serviceAccountName: timescale-timescaledb
      securityContext:
        # The postgres user inside the TimescaleDB image has uid=1000.
        # This configuration ensures the permissions of the mounts are suitable
        fsGroup: 1000
        runAsGroup: 1000
        runAsNonRoot: true
        runAsUser: 1000
      initContainers:
      - name: tstune
        securityContext:
          allowPrivilegeEscalation: false
        image: "timescale/timescaledb-ha:pg14.6-ts2.9.1-p1"
        env:
        - name: TSTUNE_FILE
          value: /var/run/postgresql/timescaledb.conf
        - name: WAL_VOLUME_SIZE
          value: 1Gi
        - name: DATA_VOLUME_SIZE
          value: 2Gi
        - name: RESOURCES_CPU_REQUESTS
          valueFrom:
            resourceFieldRef:
              containerName: timescaledb
              resource: requests.cpu
              divisor: "1"
        - name: RESOURCES_MEMORY_REQUESTS
          valueFrom:
            resourceFieldRef:
              containerName: timescaledb
              resource: requests.memory
              divisor: 1Mi
        - name: RESOURCES_CPU_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: timescaledb
              resource: limits.cpu
              divisor: "1"
        - name: RESOURCES_MEMORY_LIMIT
          valueFrom:
            resourceFieldRef:
              containerName: timescaledb
              resource: limits.memory
              divisor: 1Mi
        # Command below will run the timescaledb-tune utility and configure min/max wal size based on PVCs size
        command:
          - sh
          - "-c"
          - '/etc/timescaledb/scripts/tstune.sh '
        volumeMounts:
        - name: socket-directory
          mountPath: /var/run/postgresql
        - name: timescaledb-scripts
          mountPath: /etc/timescaledb/scripts
          readOnly: true
        resources:
          
          {}
      # Issuing the final checkpoints on a busy database may take considerable time.
      # Unfinished checkpoints will require more time during startup, so the tradeoff
      # here is time spent in shutdown/time spent in startup.
      # We choose shutdown here, especially as during the largest part of the shutdown
      # we can still serve clients.
      terminationGracePeriodSeconds: 600
      containers:
      - name: timescaledb
        securityContext:
          allowPrivilegeEscalation: false
        image: "timescale/timescaledb-ha:pg14.6-ts2.9.1-p1"
        imagePullPolicy: Always
        lifecycle:
          preStop:
            exec:
              command:
              - psql
              - -X
              - --file
              - "/etc/timescaledb/scripts/lifecycle_preStop.sql"
        # When reusing an already existing volume it sometimes happens that the permissions
        # of the PGDATA and/or wal directory are incorrect. To guard against this, we always correctly
        # set the permissons of these directories before we hand over to Patroni.
        # We also create all the tablespaces that are defined, to ensure a smooth restore/recovery on a
        # pristine set of Volumes.
        # As PostgreSQL requires to have full control over the permissions of the tablespace directories,
        # we create a subdirectory "data" in every tablespace mountpoint. The full path of every tablespace
        # therefore always ends on "/data".
        # By creating a .pgpass file in the $HOME directory, we expose the superuser password
        # to processes that may not have it in their environment (like the preStop lifecycle hook).
        # To ensure Patroni will not mingle with this file, we give Patroni its own pgpass file.
        # As these files are in the $HOME directory, they are only available to *this* container,
        # and they are ephemeral.
        command:
          - /bin/bash
          - "-c"
          - |
            
            install -o postgres -g postgres -d -m 0700 "/var/lib/postgresql/data" "/var/lib/postgresql/wal/pg_wal" || exit 1
            TABLESPACES=""
            for tablespace in ; do
              install -o postgres -g postgres -d -m 0700 "/var/lib/postgresql/tablespaces/${tablespace}/data"
            done

            # Environment variables can be read by regular users of PostgreSQL. Especially in a Kubernetes
            # context it is likely that some secrets are part of those variables.
            # To ensure we expose as little as possible to the underlying PostgreSQL instance, we have a list
            # of allowed environment variable patterns to retain.
            #
            # We need the KUBERNETES_ environment variables for the native Kubernetes support of Patroni to work.
            #
            # NB: Patroni will remove all PATRONI_.* environment variables before starting PostgreSQL

            # We store the current environment, as initscripts, callbacks, archive_commands etc. may require
            # to have the environment available to them
            set -o posix
            export -p > "${HOME}/.pod_environment"
            export -p | grep PGBACKREST > "${HOME}/.pgbackrest_environment"

            for UNKNOWNVAR in $(env | awk -F '=' '!/^(PATRONI_.*|HOME|PGDATA|PGHOST|LC_.*|LANG|PATH|KUBERNETES_SERVICE_.*|AWS_ROLE_ARN|AWS_WEB_IDENTITY_TOKEN_FILE)=/ {print $1}')
            do
                unset "${UNKNOWNVAR}"
            done

            touch /var/run/postgresql/timescaledb.conf
            touch /var/run/postgresql/wal_status

            echo "*:*:*:postgres:${PATRONI_SUPERUSER_PASSWORD}" >> ${HOME}/.pgpass
            chmod 0600 ${HOME}/.pgpass

            export PATRONI_POSTGRESQL_PGPASS="${HOME}/.pgpass.patroni"

            exec patroni /etc/timescaledb/patroni.yaml
        env:
        # We use mixed case environment variables for Patroni User management,
        # as the variable themselves are documented to be PATRONI_<username>_OPTIONS.
        # Where possible, we want to have lowercase usernames in PostgreSQL as more complex postgres usernames
        # requiring quoting to be done in certain contexts, which many tools do not do correctly, or even at all.
        # https://patroni.readthedocs.io/en/latest/ENVIRONMENT.html#bootstrap-configuration
        - name: PATRONICTL_CONFIG_FILE
          value: "/etc/timescaledb/patroni.yaml"
        - name: PATRONI_admin_OPTIONS
          value: createrole,createdb
        - name: PATRONI_REPLICATION_USERNAME
          value: standby
        # To specify the PostgreSQL and Rest API connect addresses we need
        # the PATRONI_KUBERNETES_POD_IP to be available as a bash variable, so we can compose an
        # IP:PORT address later on
        - name: PATRONI_KUBERNETES_POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: PATRONI_POSTGRESQL_CONNECT_ADDRESS
          value: "$(PATRONI_KUBERNETES_POD_IP):5432"
        - name: PATRONI_RESTAPI_CONNECT_ADDRESS
          value: "$(PATRONI_KUBERNETES_POD_IP):8008"
        - name: PATRONI_KUBERNETES_PORTS
          value: '[{"name": "postgresql", "port": 5432}]'
        - name: PATRONI_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: PATRONI_POSTGRESQL_DATA_DIR
          value: "/var/lib/postgresql/data"
        - name: PATRONI_KUBERNETES_NAMESPACE
          value: cityos
        - name: PATRONI_KUBERNETES_LABELS
          value: "{app: timescale-timescaledb, cluster-name: timescale-timescaledb, release: timescale}"
        - name: PATRONI_SCOPE
          value: timescale-timescaledb
        - name: PGBACKREST_CONFIG
          value: /etc/pgbackrest/pgbackrest.conf
        # PGDATA and PGHOST are not required to let Patroni/PostgreSQL run correctly,
        # but for interactive sessions, callbacks and PostgreSQL tools they should be correct.
        - name: PGDATA
          value: "$(PATRONI_POSTGRESQL_DATA_DIR)"
        - name: PGHOST
          value: "/var/run/postgresql"
        - name: WALDIR
          value: "/var/lib/postgresql/wal/pg_wal"
        - name: BOOTSTRAP_FROM_BACKUP
          value: "0"
        - name: PGBACKREST_BACKUP_ENABLED
          value: "false"
        - name: TSTUNE_FILE
          value: /var/run/postgresql/timescaledb.conf
          # pgBackRest is also called using the archive_command if the backup is enabled.
          # this script will also need access to the environment variables specified for
          # the backup. This can be removed once we do not directly invoke pgBackRest
          # from inside the TimescaleDB container anymore
        envFrom:
        - secretRef:
            name: "timescale-timescaledb-credentials"
            optional: false
        - secretRef:
            name: "timescale-timescaledb-pgbackrest"
            optional: true
        ports:
        - containerPort: 8008
          name: patroni
        - containerPort: 5432
          name: postgresql
        readinessProbe:
          exec:
            command:
              - pg_isready
              - -h
              - /var/run/postgresql
          initialDelaySeconds: 5
          periodSeconds: 30
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 6
        volumeMounts:
        - name: storage-volume
          mountPath: "/var/lib/postgresql"
          subPath: ""
        - name: wal-volume
          mountPath: "/var/lib/postgresql/wal"
          subPath: ""
        - mountPath: /etc/timescaledb/patroni.yaml
          subPath: patroni.yaml
          name: patroni-config
          readOnly: true
        - mountPath: /etc/timescaledb/scripts
          name: timescaledb-scripts
          readOnly: true
        - mountPath: /etc/pgbackrest_secrets
          name: pgbackrest-secrets
          readOnly: true
        - mountPath: "/etc/timescaledb/post_init.d"
          name: post-init
          readOnly: true
        - mountPath: /etc/certificate
          name: certificate
          readOnly: true
        - name: socket-directory
          mountPath: /var/run/postgresql
        
        - mountPath: /etc/pgbackrest
          name: pgbackrest
          readOnly: true
        - mountPath: /etc/pgbackrest/bootstrap
          name: pgbackrest-bootstrap
          readOnly: true
        resources:
          
          {}
      volumes:
      - name: socket-directory
        emptyDir: {}
      - name: patroni-config
        configMap:
          name: timescale-timescaledb-patroni
      - name: timescaledb-scripts
        configMap:
          name: timescale-timescaledb-scripts
          defaultMode: 488 # 0750 permissions
      
      - name: post-init
        projected:
          defaultMode: 0750
          sources:
            
            - configMap:
                name: custom-init-scripts
                optional: true
            - secret:
                name: custom-secret-scripts
                optional: true
      - name: pgbouncer
        configMap:
          name: timescale-timescaledb-pgbouncer
          defaultMode: 416 # 0640 permissions
          optional: true
      - name: pgbackrest
        configMap:
          name: timescale-timescaledb-pgbackrest
          defaultMode: 416 # 0640 permissions
          optional: true
      - name: pgbackrest-secrets
        secret:
          secretName: timescale-timescaledb-pgbackrest-secrets
          defaultMode: 416
          optional: true
      - name: certificate
        secret:
          secretName: "timescale-timescaledb-certificate"
          defaultMode: 416 # 0640 permissions
      - name: pgbackrest-bootstrap
        secret:
          secretName: pgbackrest-bootstrap
          optional: True
  volumeClaimTemplates:
    - metadata:
        name: storage-volume
        annotations:
        labels:
          app: timescale-timescaledb
          release: timescale
          heritage: Helm
          cluster-name: timescale-timescaledb
          purpose: data-directory
      spec:
        accessModes:
          
        - ReadWriteOnce
        resources:
          requests:
            storage: "2Gi"
    - metadata:
        name: wal-volume
        annotations:
        labels:
          app: timescale-timescaledb
          release: timescale
          heritage: Helm
          cluster-name: timescale-timescaledb
          purpose: wal-directory
      spec:
        accessModes:
          
        - ReadWriteOnce
        resources:
          requests:
            storage: "1Gi"
---
# Source: timescaledb-single/templates/configmap-pgbackrest.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
---
# Source: timescaledb-single/templates/configmap-pgbouncer.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
---
# Source: timescaledb-single/templates/pgbackrest.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
---
# Source: timescaledb-single/templates/secret-certificate.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: v1
kind: Secret
metadata:
  name: "timescale-timescaledb-certificate"
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: certificates
  annotations:
    "helm.sh/hook": pre-install,post-delete
    "helm.sh/hook-weight": "0"
type: kubernetes.io/tls
stringData:
  tls.crt: "-----BEGIN CERTIFICATE-----\nMIIDLDCCAhSgAwIBAgIRAOe69pwFJP96wCeaOf2UmcAwDQYJKoZIhvcNAQELBQAw\nIDEeMBwGA1UEAxMVdGltZXNjYWxlLXRpbWVzY2FsZWRiMB4XDTIzMDgxMzE4MjQx\nOFoXDTI4MDgxMjE4MjQxOFowIDEeMBwGA1UEAxMVdGltZXNjYWxlLXRpbWVzY2Fs\nZWRiMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA3uIMAHWFWTJTqrrB\ndjgTUFf4EOPVfJT2KJiz5l1nDm/nGjXi9AwEGof97Cw7TmvJKXuwAKYy6RzCoqtz\n/jzswVaFCtkkahfweuPga5jHCnrEwxzRImxKtDA469zt43i32Eygh1I60TNru3XO\nqgw+ppQeiUEEGrOo4lSBdXgbdwQEjogRmoJ1dVaFywSTsK7F4xC3Jn+VHSo1XvSz\nRcLOYpY7sM6P3SWMCKH0ZJRa/w4slRJcDOqAKkK3Y7Ky56inrNvQyK4cZITKmwAk\nOAajUrNN8RGKKcm1Zja2f7v34zBNHfOygA4G18tM7vbt/I0c6KMMVFezbOsWVHOz\nk0cAVwIDAQABo2EwXzAOBgNVHQ8BAf8EBAMCAqQwHQYDVR0lBBYwFAYIKwYBBQUH\nAwEGCCsGAQUFBwMCMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYEFAA55qJ+jLV9\n/E55m25HVUArLJNaMA0GCSqGSIb3DQEBCwUAA4IBAQDEeNiQ/rjZhtmdI/V6T36k\n8SI5hz7JSvM4HkgYQq0sKsNfb6RodKmjd9HVPXMTxbcFRQawIxVvzd9xAyLWVvtv\n6my6o0p4JOnG9YLlvZQfgFQ/pPrRVbg+SSR1gh1bnEdq6MnjLhqe4qOgfnki3CE0\n6tDDpg6FJKU9BglyxHh14yLlMiIJ+dYzLCPHuSlpUp2AoBhbBOMYBVMirjLU6WGc\nWuoWCQ4H7Mtw1kcrhecrrorQ0wQ1hY+i0cm6Uwd4nFYwAWP35FIChQ3rB2O9bLaT\nABF+ewcrjuLrfOMrMxhC9+6x2Tb9d+DkxxFaSHotpoGryxo6sSqarA+GcIC0FJJF\n-----END CERTIFICATE-----\n"
  tls.key: "-----BEGIN RSA PRIVATE KEY-----\nMIIEowIBAAKCAQEA3uIMAHWFWTJTqrrBdjgTUFf4EOPVfJT2KJiz5l1nDm/nGjXi\n9AwEGof97Cw7TmvJKXuwAKYy6RzCoqtz/jzswVaFCtkkahfweuPga5jHCnrEwxzR\nImxKtDA469zt43i32Eygh1I60TNru3XOqgw+ppQeiUEEGrOo4lSBdXgbdwQEjogR\nmoJ1dVaFywSTsK7F4xC3Jn+VHSo1XvSzRcLOYpY7sM6P3SWMCKH0ZJRa/w4slRJc\nDOqAKkK3Y7Ky56inrNvQyK4cZITKmwAkOAajUrNN8RGKKcm1Zja2f7v34zBNHfOy\ngA4G18tM7vbt/I0c6KMMVFezbOsWVHOzk0cAVwIDAQABAoIBADRJ1cT5zSnq26QP\nIjno1+cebC0SNlB+5e0KPhJKUVYavJ3yjGbytre+tbyFd/hi4xOfV+AfHYyA2cyt\n6o13n5eQA7BS1u8o5JsAAPNdKTz1Jlob1Zh8zV3OflwfPZQDCyZse7Cjgvb3WEvn\nTG+qZbxQvYU3wo29Hl7SsnhrkA3wbEdI2QXC6HQkM2JAoGDoDcamIPqOEtgKaVpP\nuFs0Bgss6k/K0Az+H9e3kNDB3plahcFNL+th4T0fmyxT+nwRfa3fU0U7dKJeCQdZ\nAyrBOtdYoiWfUOp9nUmmDpVVsBTV/kKQ8jdnvEJ95h8TNxvDlLT6GU7zSlkF4Q8V\nJyvq6QkCgYEA8YuAIJGNtfm47oaNVYQPHQJ6xrfFR3E455QZl72ZJ2R516t9dM8r\nBR9GECyDsRfQk4APN98VNIqMI67UpPDgh+L4gCjUfe3qsaxjVqbTcChXurkb9Dqs\nm34u9yE5NKyEDIOYhDXoX980EizIN+XvJLKEaXM2yku2IQ4t63yowOMCgYEA7Dik\nqq1HslyNOz/vzEbS/4fsgsc5ShcEYPHqoZosEADgJDRirKpoAZPm+uh7PKxDSmUr\nFPwAeel13ln0ItfDLuaxbY20qEX4u1EupuiocS261yol/wBWiDpCyTg+X1Xw6sLD\nZjqpupTaJ2lDgSx3svg//7ikNfdTyxVAEPdMIP0CgYB3KAQ7DNLFtL2AHaxXRWBX\nRUPgCAQwLDCQPUUq0ANfpXdd52FqYFCupDDiBbOph0ADo6bePxSs6LQdXj/UZme9\nYUCJqJq6zMV8hLg0pPOlXqF4oZyirNc9ldABZchcrcYujjSM9DMAKJH9bU4QFUQV\n7GLZylZ7EUJ1qBMLGmH9VQKBgQDNpD4N643YA2Rb6Tw77IgxNkY6T29bRZBlSRUG\nnC2xvOFkA0rHoA2tGfHiClOGrTxjXpFT0lIjz47yBANZ9dWQVfquMcfNGiUABRuE\n6FQDO1lFiQbM6KFsNLQBcnoyCqIR7X2n2Hf72yvTVXsTz/9giWdFmpsx8985k4VT\nf8YBjQKBgHSTU7mR+C8k4B5JZLLB0s4TUubwaH5sotUTzgkk6KEnA2/Mr2EhWy9G\nWsan6f1Hm1au0dK/IhX7hqhM/yk+UYC/b2NndsA9rZ/7PYgCPRpA6YAQKZRmohif\nHzzLMMkd2iKGwkCvJI1nuHxbqUCivaXf4BhRObqBfaEbg8u1CRRQ\n-----END RSA PRIVATE KEY-----\n"
...
---
# Source: timescaledb-single/templates/secret-patroni.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: v1
kind: Secret
metadata:
  name: "timescale-timescaledb-credentials"
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: patroni
  annotations:
    "helm.sh/hook": pre-install,post-delete
    "helm.sh/hook-weight": "0"
    "helm.sh/resource-policy": keep
type: Opaque
stringData:
  PATRONI_SUPERUSER_PASSWORD: "IfrPnnPgPLtbyJmU"
  PATRONI_REPLICATION_PASSWORD: "4dIbZM5IzNbCYKg7"
  PATRONI_admin_PASSWORD: "ysBFuM8WGT1EWjs3"
...
---
# Source: timescaledb-single/templates/secret-pgbackrest.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.
apiVersion: v1
kind: Secret
metadata:
  name: "timescale-timescaledb-pgbackrest"
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: pgbackrest
  annotations:
    "helm.sh/hook": pre-install,post-delete
    "helm.sh/hook-weight": "0"
    "helm.sh/resource-policy": keep
type: Opaque
stringData:
  {}
...
---
# Source: timescaledb-single/templates/job-update-patroni.yaml
# This file and its contents are licensed under the Apache License 2.0.
# Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

apiVersion: batch/v1
kind: Job
metadata:
  name: "timescale-timescaledb-patroni-1"
  namespace: cityos
  labels:
    
    app: timescale-timescaledb
    chart: timescaledb-single-0.33.1
    release: timescale
    heritage: Helm
    cluster-name: timescale-timescaledb
    app.kubernetes.io/name: "timescale-timescaledb"
    app.kubernetes.io/version: 0.33.1
    app.kubernetes.io/component: patroni
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  activeDeadlineSeconds: 120
  template:
    metadata:
      labels:
        
        app: timescale-timescaledb
        chart: timescaledb-single-0.33.1
        release: timescale
        heritage: Helm
        cluster-name: timescale-timescaledb
        app.kubernetes.io/name: "timescale-timescaledb"
        app.kubernetes.io/version: 0.33.1
    spec:
      restartPolicy: OnFailure
      containers:
      - name: timescale-timescaledb-patch-patroni-config
        image: "curlimages/curl:7.87.0"
        imagePullPolicy: Always
        command: ["/bin/sh"]
        # Patching the Patroni configuration is good, however it should not block an upgrade from going through
        # Therefore we ensure we always exit with an exitcode 0, so that Helm is satisfied with this upgrade job
        args:
        - '-c'
        - |
          /usr/bin/curl --connect-timeout 30 --include --request PATCH --data \
          "{\"loop_wait\":10,\"maximum_lag_on_failover\":33554432,\"postgresql\":{\"parameters\":{\"archive_command\":\"/etc/timescaledb/scripts/pgbackrest_archive.sh %p\",\"archive_mode\":\"on\",\"archive_timeout\":\"1800s\",\"autovacuum_analyze_scale_factor\":0.02,\"autovacuum_max_workers\":10,\"autovacuum_naptime\":\"5s\",\"autovacuum_vacuum_cost_limit\":500,\"autovacuum_vacuum_scale_factor\":0.05,\"hot_standby\":\"on\",\"log_autovacuum_min_duration\":\"1min\",\"log_checkpoints\":\"on\",\"log_connections\":\"on\",\"log_disconnections\":\"on\",\"log_line_prefix\":\"%t [%p]: [%c-%l] %u@%d,app=%a [%e] \",\"log_lock_waits\":\"on\",\"log_min_duration_statement\":\"1s\",\"log_statement\":\"ddl\",\"max_connections\":100,\"max_prepared_transactions\":150,\"shared_preload_libraries\":\"timescaledb,pg_stat_statements\",\"ssl\":\"on\",\"ssl_cert_file\":\"/etc/certificate/tls.crt\",\"ssl_key_file\":\"/etc/certificate/tls.key\",\"tcp_keepalives_idle\":900,\"tcp_keepalives_interval\":100,\"temp_file_limit\":\"1GB\",\"timescaledb.passfile\":\"../.pgpass\",\"unix_socket_directories\":\"/var/run/postgresql\",\"unix_socket_permissions\":\"0750\",\"wal_level\":\"hot_standby\",\"wal_log_hints\":\"on\"},\"use_pg_rewind\":true,\"use_slots\":true},\"retry_timeout\":10,\"ttl\":30}" \
          "http://timescale-timescaledb-config:8008/config"
          exit 0

---
# Source: cityos-api/charts/keycloak/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cityos-keycloak
  namespace: "cityos"
  labels:
    app.kubernetes.io/name: keycloak
    helm.sh/chart: keycloak-15.0.0
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: keycloak
automountServiceAccountToken: true
---
# Source: cityos-api/charts/keycloak/charts/postgresql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: cityos-postgresql
  namespace: "cityos"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-12.4.2
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  postgres-password: "YXV0aA=="
  password: "YXV0aA=="
  # We don't auto-generate LDAP password when it's not provided as we do for other passwords
---
# Source: cityos-api/charts/keycloak/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: cityos-keycloak
  namespace: "cityos"
  labels:
    app.kubernetes.io/name: keycloak
    helm.sh/chart: keycloak-15.0.0
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: keycloak
type: Opaque
data:
  admin-password: "eW91cnBhc3N3b3Jk"
---
# Source: cityos-api/charts/keycloak/templates/configmap-env-vars.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cityos-keycloak-env-vars
  namespace: "cityos"
  labels:
    app.kubernetes.io/name: keycloak
    helm.sh/chart: keycloak-15.0.0
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: keycloak
data:
  KEYCLOAK_ADMIN: "admin"
  KEYCLOAK_HTTP_PORT: "8080"
  KEYCLOAK_PROXY: "edge"
  KEYCLOAK_ENABLE_STATISTICS: "false"
  KEYCLOAK_DATABASE_HOST: "cityos-postgresql"
  KEYCLOAK_DATABASE_PORT: "5432"
  KEYCLOAK_DATABASE_NAME: "keycloak"
  KEYCLOAK_DATABASE_USER: "auth"
  KEYCLOAK_PRODUCTION:  "true"
  KEYCLOAK_ENABLE_HTTPS: "false"
  KEYCLOAK_CACHE_TYPE: "ispn"
  KEYCLOAK_CACHE_STACK: "kubernetes"
  JAVA_OPTS_APPEND: "-Djgroups.dns.query=cityos-keycloak-headless.cityos.svc.cluster.local"
  KEYCLOAK_LOG_OUTPUT: "default"
  KC_LOG_LEVEL: "INFO"
---
# Source: cityos-api/charts/keycloak/templates/keycloak-config-cli-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cityos-keycloak-keycloak-config-cli-configmap
  namespace: "cityos"
  labels:
    app.kubernetes.io/name: keycloak
    helm.sh/chart: keycloak-15.0.0
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: keycloak-config-cli
data:
  realm.json: |
    {
    "id": "citydashboard",
    "realm": "citydashboard",
    "enabled": true,
    "roles": {
        "realm": [
        {
            "name": "ROLE_user",
            "composite": false,
            "clientRole": false,
            "containerId": "citydashboard",
            "attributes": {}
        },
        {
            "name": "ROLE_admin",
            "composite": false,
            "clientRole": false,
            "containerId": "citydashboard",
            "attributes": {}
        },
        {
            "name": "ROLE_reader",
            "composite": false,
            "clientRole": false,
            "containerId": "citydashboard",
            "attributes": {}
        }
        ],
        "client": {
        "citydashboard": []
        }
    },
    "groups": [
        {
        "name": "public",
        "path": "/public",
        "attributes": {},
        "realmRoles": [
            "ROLE_reader"
        ],
        "clientRoles": {},
        "subGroups": []
        }
    ],
    "defaultGroups": [
        "/public"
    ],
    "clients": [
        {
        "clientId": "citydashboard",
        "name": "citydashboard",
        "rootUrl": "",
        "baseUrl": "",
        "surrogateAuthRequired": false,
        "enabled": true,
        "alwaysDisplayInConsole": false,
        "clientAuthenticatorType": "client-secret",
        "redirectUris": [
            "*"
        ],
        "webOrigins": [
            "*"
        ],
        "notBefore": 0,
        "bearerOnly": false,
        "consentRequired": false,
        "standardFlowEnabled": true,
        "implicitFlowEnabled": false,
        "directAccessGrantsEnabled": false,
        "serviceAccountsEnabled": false,
        "publicClient": true,
        "frontchannelLogout": false,
        "protocol": "openid-connect",
        "attributes": {
            "id.token.as.detached.signature": "false",
            "saml.assertion.signature": "false",
            "saml.force.post.binding": "false",
            "saml.multivalued.roles": "false",
            "saml.encrypt": "false",
            "login_theme": "keycloak",
            "oauth2.device.authorization.grant.enabled": "false",
            "backchannel.logout.revoke.offline.tokens": "false",
            "saml.server.signature": "false",
            "saml.server.signature.keyinfo.ext": "false",
            "use.refresh.tokens": "true",
            "exclude.session.state.from.auth.response": "false",
            "oidc.ciba.grant.enabled": "false",
            "saml.artifact.binding": "false",
            "backchannel.logout.session.required": "true",
            "client_credentials.use_refresh_token": "false",
            "saml_force_name_id_format": "false",
            "require.pushed.authorization.requests": "false",
            "saml.client.signature": "false",
            "tls.client.certificate.bound.access.tokens": "false",
            "saml.authnstatement": "false",
            "display.on.consent.screen": "false",
            "saml.onetimeuse.condition": "false"
        },
        "authenticationFlowBindingOverrides": {},
        "fullScopeAllowed": true,
        "nodeReRegistrationTimeout": -1,
        "protocolMappers": [
            {
            "name": "groups",
            "protocol": "openid-connect",
            "protocolMapper": "oidc-group-membership-mapper",
            "consentRequired": false,
            "config": {
                "full.path": "false",
                "id.token.claim": "false",
                "access.token.claim": "true",
                "claim.name": "groups",
                "userinfo.token.claim": "true"
            }
            }
        ],      
        "defaultClientScopes": [
            "web-origins",
            "roles",
            "profile",
            "email"
        ],
        "optionalClientScopes": [
            "address",
            "phone",
            "offline_access",
            "microprofile-jwt"
        ]
        }
    ]
    }
---
# Source: cityos-api/charts/keycloak/charts/postgresql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: cityos-postgresql-hl
  namespace: "cityos"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-12.4.2
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
    # Use this annotation in addition to the actual publishNotReadyAddresses
    # field below because the annotation will stop being respected soon but the
    # field is broken in some versions of Kubernetes:
    # https://github.com/kubernetes/kubernetes/issues/58662
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  type: ClusterIP
  clusterIP: None
  # We want all pods in the StatefulSet to have their addresses published for
  # the sake of the other Postgresql pods even before they're ready, since they
  # have to be able to talk to each other in order to become ready.
  publishNotReadyAddresses: true
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/component: primary
---
# Source: cityos-api/charts/keycloak/charts/postgresql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: cityos-postgresql
  namespace: "cityos"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-12.4.2
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: tcp-postgresql
      port: 5432
      targetPort: tcp-postgresql
      nodePort: null
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/component: primary
---
# Source: cityos-api/charts/keycloak/templates/headless-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: cityos-keycloak-headless
  namespace: "cityos"
  labels:
    app.kubernetes.io/name: keycloak
    helm.sh/chart: keycloak-15.0.0
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: keycloak
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/component: keycloak
---
# Source: cityos-api/charts/keycloak/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: cityos-keycloak
  namespace: "cityos"
  labels:
    app.kubernetes.io/name: keycloak
    helm.sh/chart: keycloak-15.0.0
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: keycloak
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
      nodePort: null
  selector:
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/component: keycloak
---
# Source: cityos-api/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: cityos-cityos-api
  labels:
    helm.sh/chart: cityos-api-1.1.10
    app.kubernetes.io/name: cityos-api
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/version: "develop_v2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: cityos-api
    app.kubernetes.io/instance: cityos
---
# Source: cityos-api/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cityos-cityos-api
  labels:
    helm.sh/chart: cityos-api-1.1.10
    app.kubernetes.io/name: cityos-api
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/version: "develop_v2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: cityos-api
      app.kubernetes.io/instance: cityos
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cityos-api
        app.kubernetes.io/instance: cityos
    spec:
      imagePullSecrets:
        - name: ghcr-starwit
      serviceAccountName: default
      securityContext:
        null

      containers:
        - name: cityos-api
          securityContext:
            null
          image: "ghcr.io/starwit/city-demos/backend:develop_v2"
          imagePullPolicy: Always
          env:
            - name: SPRING_PROFILES_ACTIVE
              value: "prod"
            - name: SPRING_DATASOURCE_URL
              value: jdbc:postgresql://timescale-timescaledb:5432/citydashboard?useLegacyDatetimeCode=false&serverTimezone=CET
            - name: SPRING_DATASOURCE_USERNAME
              value: citydashboard
            - name: SPRING_DATASOURCE_PASSWORD
              value: citydashboard
            - name: SERVER_SERVLET_CONTEXT_PATH
              value: /cityos-cityos-api
            - name: KEYCLOAK_AUTH-SERVER-URL
              value: http://cityos-keycloak-http/cityos-auth
            - name: SERVER_USE_FORWARD_HEADERS 
              value: "true"
            - name: SERVER_FORWARD_HEADERS_STRATEGY
              value: FRAMEWORK
            - name: KEYCLOAK_REALM
              value: 
            - name: KEYCLOAK_RESOURCE
              value: 
            - name: ENDPOINT_TRAFFIC
              value: http://anett-xmg-fusion-15-xfu15l19/traffic/api
            - name: ENDPOINT_GRAFANA
              value: http://anett-xmg-fusion-15-xfu15l19/grafana
            - name: ENDPOINT_AUTH_ENABLE
              value: "true"
            - name: CITYOS_AUTHENTICATION_URI
              value: http://anett-xmg-fusion-15-xfu15l19/cityos-auth/realms/citydashboard
            - name: VW2_CLIENTID
              value: clientid
            - name: VW2_CLIENT_SECRET
              value: secret
            - name: VW2_DATACAPTURERSTATUS
              value: https://carmel.vw2-demospace.de/vw2datacapture/api/datapoint
            - name: VW2_DATACAPTURERCAMURL
              value: https://carmel.vw2-demospace.de/vw2datacapture/api/cameras
            - name: VW2_AUTHURL
              value: https://carmel2.vw2-demospace.de/carmel-smart-cities-auth/realms/icv/protocol/openid-connect/token
            - name: VW2_CAMERASERVICEURL
              value: https://carmel2.vw2-demospace.de/carmel-smart-cities-cameraservice/v1/cameras
            
          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /cityos-cityos-api/monitoring/health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 5
          readinessProbe:
            httpGet:
              path: /cityos-cityos-api/monitoring/health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 5
          resources:
            null
---
# Source: cityos-api/charts/keycloak/charts/postgresql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cityos-postgresql
  namespace: "cityos"
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-12.4.2
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  serviceName: cityos-postgresql-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/instance: cityos
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: cityos-postgresql
      labels:
        app.kubernetes.io/name: postgresql
        helm.sh/chart: postgresql-12.4.2
        app.kubernetes.io/instance: cityos
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: primary
    spec:
      serviceAccountName: default
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: postgresql
                    app.kubernetes.io/instance: cityos
                    app.kubernetes.io/component: primary
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      hostNetwork: false
      hostIPC: false
      containers:
        - name: postgresql
          image: docker.io/bitnami/postgresql:15.2.0-debian-11-r26
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: POSTGRESQL_PORT_NUMBER
              value: "5432"
            - name: POSTGRESQL_VOLUME_DIR
              value: "/bitnami/postgresql"
            - name: PGDATA
              value: "/bitnami/postgresql/data"
            # Authentication
            - name: POSTGRES_USER
              value: "auth"
            - name: POSTGRES_POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: cityos-postgresql
                  key: postgres-password
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: cityos-postgresql
                  key: password
            - name: POSTGRES_DB
              value: "keycloak"
            # Replication
            # Initdb
            # Standby
            # LDAP
            - name: POSTGRESQL_ENABLE_LDAP
              value: "no"
            # TLS
            - name: POSTGRESQL_ENABLE_TLS
              value: "no"
            # Audit
            - name: POSTGRESQL_LOG_HOSTNAME
              value: "false"
            - name: POSTGRESQL_LOG_CONNECTIONS
              value: "false"
            - name: POSTGRESQL_LOG_DISCONNECTIONS
              value: "false"
            - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
              value: "off"
            # Others
            - name: POSTGRESQL_CLIENT_MIN_MESSAGES
              value: "error"
            - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
              value: "pgaudit"
          ports:
            - name: tcp-postgresql
              containerPort: 5432
          livenessProbe:
            failureThreshold: 6
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - exec pg_isready -U "auth" -d "dbname=keycloak" -h 127.0.0.1 -p 5432
          readinessProbe:
            failureThreshold: 6
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 5
            exec:
              command:
                - /bin/sh
                - -c
                - -e
                
                - |
                  exec pg_isready -U "auth" -d "dbname=keycloak" -h 127.0.0.1 -p 5432
                  [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized ]
          resources:
            limits: {}
            requests:
              cpu: 250m
              memory: 256Mi
          volumeMounts:
            - name: dshm
              mountPath: /dev/shm
            - name: data
              mountPath: /bitnami/postgresql
      volumes:
        - name: dshm
          emptyDir:
            medium: Memory
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: cityos-api/charts/keycloak/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cityos-keycloak
  namespace: "cityos"
  labels:
    app.kubernetes.io/name: keycloak
    helm.sh/chart: keycloak-15.0.0
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: keycloak
spec:
  replicas: 1
  podManagementPolicy: Parallel
  serviceName: cityos-keycloak-headless
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels: 
      app.kubernetes.io/name: keycloak
      app.kubernetes.io/instance: cityos
      app.kubernetes.io/component: keycloak
  template:
    metadata:
      annotations:
        checksum/configmap-env-vars: c4c50175b5293b44afc1b8f8ac20d565ff24aa62934db4660c0d983416d565df
        checksum/secrets: 9c64299eeaf2a757e6b96edc23ae980c6dce95b24a75689a6cafad269b852c3f
      labels:
        app.kubernetes.io/name: keycloak
        helm.sh/chart: keycloak-15.0.0
        app.kubernetes.io/instance: cityos
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: keycloak
    spec:
      serviceAccountName: cityos-keycloak
      
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: keycloak
                    app.kubernetes.io/instance: cityos
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
      enableServiceLinks: true
      initContainers:
      containers:
        - name: keycloak
          image: docker.io/bitnami/keycloak:21.1.1-debian-11-r0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: KUBERNETES_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: BITNAMI_DEBUG
              value: "false"
            - name: KEYCLOAK_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: cityos-keycloak
                  key: admin-password
            - name: KEYCLOAK_DATABASE_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: cityos-postgresql
                  key: password
            - name: KEYCLOAK_HTTP_RELATIVE_PATH
              value: "/cityos-auth/"
          envFrom:
            - configMapRef:
                name: cityos-keycloak-env-vars
          resources:
            limits: {}
            requests: {}
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
            - name: infinispan
              containerPort: 7800
              protocol: TCP
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 300
            periodSeconds: 1
            successThreshold: 1
            timeoutSeconds: 5
            httpGet:
              path: /cityos-auth/
              port: http
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 30
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            httpGet:
              path: /cityos-auth/realms/master
              port: http
          volumeMounts:
      volumes:
---
# Source: cityos-api/charts/keycloak/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: cityos-keycloak
  namespace: "cityos"
  labels:
    app.kubernetes.io/name: keycloak
    helm.sh/chart: keycloak-15.0.0
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: keycloak
  annotations:
spec:
  rules:
    - host: "anett-xmg-fusion-15-xfu15l19"
      http:
        paths:
          - path: /cityos-auth
            pathType: ImplementationSpecific
            backend:
              service:
                name: cityos-keycloak
                port:
                  name: http
---
# Source: cityos-api/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: cityos-cityos-api
  labels:
    helm.sh/chart: cityos-api-1.1.10
    app.kubernetes.io/name: cityos-api
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/version: "develop_v2"
    app.kubernetes.io/managed-by: Helm
  annotations:
    traefik.ingress.kubernetes.io/router.middlewares: letsencrypt-prod
spec:
  tls:
    - hosts:
        - "anett-xmg-fusion-15-xfu15l19"
      secretName: "host-tls"
  rules:
    - host: "anett-xmg-fusion-15-xfu15l19"
      http:
        paths:
          - path: /cityos-cityos-api
            pathType: Prefix
            backend:
              service:
                name: cityos-cityos-api
                port:
                  number: 80
---
# Source: cityos-api/charts/keycloak/templates/keycloak-config-cli-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: cityos-keycloak-keycloak-config-cli
  namespace: "cityos"
  labels:
    app.kubernetes.io/name: keycloak
    helm.sh/chart: keycloak-15.0.0
    app.kubernetes.io/instance: cityos
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: keycloak-config-cli
  annotations:
    helm.sh/hook: post-install,post-upgrade,post-rollback
    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation
    helm.sh/hook-weight: "5"
spec:
  backoffLimit: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: keycloak
        helm.sh/chart: keycloak-15.0.0
        app.kubernetes.io/instance: cityos
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: keycloak-config-cli
      annotations:
        checksum/configuration: 500f8fe352b3ceb57d4e9cf75048f896ed70858e0406171f489479438ac9c59f
    spec:
      serviceAccountName: cityos-keycloak
      
      restartPolicy: Never
      securityContext:
        fsGroup: 1001
      containers:
        - name: keycloak-config-cli
          image: docker.io/adorsys/keycloak-config-cli:latest
          imagePullPolicy: IfNotPresent
          command:
            - java
            - -jar
            - /app/keycloak-config-cli.jar
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: KEYCLOAK_URL
              value: http://cityos-keycloak-headless:8080/cityos-auth/
            - name: KEYCLOAK_USER
              value: "admin"
            - name: KEYCLOAK_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: cityos-keycloak
                  key: admin-password
            - name: IMPORT_FILES_LOCATIONS
              value: /config/*
            - name: KEYCLOAK_AVAILABILITYCHECK_ENABLED
              value: "true"
          volumeMounts:
            - name: config-volume
              mountPath: /config
          resources:
            limits: {}
            requests: {}
      volumes:
        - name: config-volume
          configMap:
            name: cityos-keycloak-keycloak-config-cli-configmap

---
# Source: cityos-frontend/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cityos-frontend
  labels:
    helm.sh/chart: cityos-frontend-0.1.2
    app.kubernetes.io/name: cityos-frontend
    app.kubernetes.io/instance: cityos-frontend
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: cityos-frontend
      app.kubernetes.io/instance: cityos-frontend
  template:
    metadata:
      labels:
        app.kubernetes.io/name: cityos-frontend
        app.kubernetes.io/instance: cityos-frontend
    spec:
      imagePullSecrets:
        - name: ghcr-starwit
      serviceAccountName: cityos-frontend
      securityContext:
        {}
      containers:
        - name: cityos-frontend
          securityContext:
            {}
          image: "ghcr.io/starwit/city-demos/frontend:develop_v2"
          imagePullPolicy: Always
          env:
            - name: API_URL
              value: http://anett-xmg-fusion-15-xfu15l19/cityos-cityos-api/
            - name: AUTH_AUTHORITY
              value: http://anett-xmg-fusion-15-xfu15l19/cityos-auth/realms/citydashboard
            - name: AUTH_CLIENT_ID
              value: citydashboard
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {}

---
# Source: cityos-frontend/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "cityos-frontend-test-connection"
  labels:
    helm.sh/chart: cityos-frontend-0.1.2
    app.kubernetes.io/name: cityos-frontend
    app.kubernetes.io/instance: cityos-frontend
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['cityos-frontend:80']
  restartPolicy: Never

---
# Source: cityos-frontend/templates/strip-prefix-middleware.yaml
# TODO: Remove on / path
apiVersion: traefik.containo.us/v1alpha1
kind: Middleware
metadata:
  name: cityos-frontend-strip-prefix
  namespace: cityos
  # No namespace defined
spec:
  stripPrefix:
    forceSlash: false
    prefixes:
      - /cityos
---
# Source: cityos-frontend/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: cityos-frontend
  labels:
    helm.sh/chart: cityos-frontend-0.1.2
    app.kubernetes.io/name: cityos-frontend
    app.kubernetes.io/instance: cityos-frontend
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    traefik.ingress.kubernetes.io/router.middlewares: cityos-cityos-frontend-strip-prefix@kubernetescrdtraefik.ingress.kubernetes.io/router.middlewares: letsencrypt-prod
spec:
  tls:
    - hosts:
        - "anett-xmg-fusion-15-xfu15l19"
      secretName: host-tls
  rules:
    - host: "anett-xmg-fusion-15-xfu15l19"
      http:
        paths:
          - path: /cityos
            pathType: Prefix
            backend:
              service:
                name: cityos-frontend
                port:
                  number: 80

---
# Source: cityos-frontend/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cityos-frontend
  labels:
    helm.sh/chart: cityos-frontend-0.1.2
    app.kubernetes.io/name: cityos-frontend
    app.kubernetes.io/instance: cityos-frontend
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm

---
# Source: cityos-frontend/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: cityos-frontend
  labels:
    helm.sh/chart: cityos-frontend-0.1.2
    app.kubernetes.io/name: cityos-frontend
    app.kubernetes.io/instance: cityos-frontend
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: cityos-frontend
    app.kubernetes.io/instance: cityos-frontend

